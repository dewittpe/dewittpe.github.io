[
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "I have developed and published several R packages and tools.\nA subset of the packages I have published are listed here in alphabetical order. Click the link to get a summary of the tool and links to the software and more details."
  },
  {
    "objectID": "software.html#cpr",
    "href": "software.html#cpr",
    "title": "Software",
    "section": "cpr",
    "text": "cpr\nControl Polygon Reduction\n\nDescription\nImplementation of the Control Polygon Reduction and Control Net Reduction methods for finding parsimonious B-spline regression models.\n\n\nLinks\n\nwebsite\ngithub"
  },
  {
    "objectID": "software.html#ensr",
    "href": "software.html#ensr",
    "title": "Software",
    "section": "ensr",
    "text": "ensr\nElastic Net Searcher\n\nDescription\nElastic net regression models are controlled by two parameters, lambda, a measure of shrinkage, and alpha, a metric defining the model’s location on the spectrum between ridge and lasso regression. glmnet provides tools for selecting lambda via cross validation but no automated methods for selection of alpha. Elastic Net SearcheR automates the simultaneous selection of both lambda and alpha. Developed, in part, with support by NICHD R03 HD094912.\n\n\nLinks\n\n[github]("
  },
  {
    "objectID": "software.html#pccc",
    "href": "software.html#pccc",
    "title": "Software",
    "section": "pccc",
    "text": "pccc\nPediatric Complex Chronic Conditions\n\nDescription\nAn implementation of the pediatric complex chronic conditions (CCC) classification system using R and C++.\n\n\nLinks\n\nCRAN\ngithub"
  },
  {
    "objectID": "software.html#pedbp",
    "href": "software.html#pedbp",
    "title": "Software",
    "section": "pedbp",
    "text": "pedbp\nPediatric Blood Pressure Percentiles and Growth Charts\n\n\nDescription\nData and utilities for estimating pediatric blood pressure percentiles by sex, age, and optionally height (stature). Additionally, CDC and WHO growth charts have been implemented for things such as weight for age, weight for stature, stature for age, and others.\n\n\nLinks\n\nwebsite\nShiny app\nR package\ngithub\nPeer reviewed manuscript in JAMA Network Open:\n\nDevelopment of a Pediatric Blood Pressure Percentile Tool for Clinical Decision Support"
  },
  {
    "objectID": "software.html#phoenix",
    "href": "software.html#phoenix",
    "title": "Software",
    "section": "phoenix",
    "text": "phoenix\nPhoenix Sepsis and Septic Shock Criteria\n\n\nDescription\nThe phoenix R package and Python module implement the Phoenix criteria for pediatric sepsis and septic shock.\n\n\nLinks\n\nwebsite\nR package\nPython module\ngithub\nPeer reviewed manuscript in JAMIA Open:\n\nphoenix: an R package and Python module for calculating the Phoenix pediatric sepsis score and criteria"
  },
  {
    "objectID": "software.html#qwraps2",
    "href": "software.html#qwraps2",
    "title": "Software",
    "section": "qwraps2",
    "text": "qwraps2\nQuick Wraps 2\n\nDescription\nA collection of tools I use for building summary tables, graphics, and general formating of reproducible reports.\n\n\nLinks\n\nwebsite\nCRAN\ngithub"
  },
  {
    "objectID": "software.html#redcapexporter",
    "href": "software.html#redcapexporter",
    "title": "Software",
    "section": "REDCapExporter",
    "text": "REDCapExporter\nREDCap Exporter\n\nDescription\nA set of tools to download data and meta data from REDCap projects via the REDCap API. The metadata is used to format the records into a nearly analysis ready data set.\n\n\nLinks\n\nwebsite\nCRAN\ngithub"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professionally I am an Assistant Research Professor in the Department of Biomedical Informatics, in the School of Medicine at the University of Colorado Anschutz Medical Campus. I work as a biostatistician, data scientist, software developer, and mentor.\nPersonally I am a Dad and woodworker."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\n\nPh.D.\n\n2017\nBiostatistics\nColorado School of Public Health\nUniversity of Colorado Anschutz Medical Campus\n\nM.S.\n\n2010\nStatistics\nColorado State University\n\nM.S.\n\n2007\nMathematics and Computer Science\nColorado School of Mines\n\nB.S.\n\nMathematics and Computer Science\n2005\nColorado School of Mines"
  },
  {
    "objectID": "posts/2017-04-03-git-diff-pdfinfo/index.html",
    "href": "posts/2017-04-03-git-diff-pdfinfo/index.html",
    "title": "git diff pdfs",
    "section": "",
    "text": "Binary files and git repositories can be a pain, especially when looking at the git diff output. Here are two helpful things to do with git to make working with .pdfs less painful.\nThe standard output from git diff when a tracked .pdf has changed is\ndiff --git a/dissertation.pdf b/dissertation.pdf\nindex 2a9ec08..d3aaa79 100644\nBinary files a/dissertation.pdf and b/dissertation.pdf differ\nNot the most helpful output. The two things I have found to be helpful are\n\nA git difftool to call diffpdf.\nHave the differences in the pdfinfo returned when calling git diff.\n\nNote, I’m writing this on a Debian machine. Set up for other Linux distributions, Winblows and Mac may differ.\nSetting up git to use diffpdf is simple. In either you project or global config file add the following lines\n[difftool \"diffpdf\"]\n  cmd = diffpdf \\\"$LOCAL\\\" \\\"$REMOTE\\\"\nFrom the command line\ngit difftool --no-prompt --tool=diffpdf dissertation.pdf\nwill open the diffpdf gui and show the differences, changes, in the dissertation.pdf file. I find this to be the most helpful when comparing different versions of the .pdf file with/for non-git users. For example, version-0.9.1 versus the current version\ngit difftool --no-prompt --tool=diffpdf version-0.9.1:dissertation.pdf dissertation.pdf\nNow, from time to time, I know that the .pdf changes are limited, or can easily be assess by looking at the metadata. To have the output of git diff show the changes in the pdf metadata do the following.\nAdd, or add to, .gitattributes file. Setting one up for all your repos is done by adding the following to you global git config file\ngit config --global core.attributesfile ~/.gitattributes\necho \"*.pdf diff=diffpdfinfo\" &gt;&gt; ~/.gitattributes\nIf you only want this to work in one particular repository you need only create the .gitattributes file in the project root directory.\nWe need to write the diffpdfinfo script and save it somewhere in your PATH.\n#!/bin/bash\n\npdfinfo $1 &gt; .localpdfmetadata\npdfinfo $2 &gt; .remotepdfmetadata\n\ndiff .localpdfmetadata .remotepdfmetadata\nRemember to made the script executable via chmod a+x.\nNow git diff has meaningful output for the .pdf file. For example, a quick recompile of the .pdf document results in the diffpdf tool telling me the two files are the same. However, git diff tells me they are different, specifically in that the creation date and modification dates have change.\n$ git diff\n\ndiff --git a/dissertation.pdf b/dissertation.pdf\nindex 2a9ec08..edd66d8 100644\n--- a/dissertation.pdf\n+++ b/dissertation.pdf\n@@ -4,8 +4,8 @@ Keywords:\n Author:\n Creator:        LaTeX with hyperref package\n Producer:       pdfTeX-1.40.15\n-CreationDate:   Wed Mar 29 14:55:27 2017\n-ModDate:        Wed Mar 29 14:55:27 2017\n+CreationDate:   Mon Apr  3 00:41:05 2017\n+ModDate:        Mon Apr  3 00:41:05 2017\n Tagged:         no\n UserProperties: no\n Suspects:       no\nDiffing a draft version to the current version there should be some changes in the metadata. The creation and modification dates have changed, but so has the number of pages and the file size.\n$ git diff version-0.9.1:dissertation.pdf dissertation.pdf\n\ndiff --git a/dissertation.pdf b/dissertation.pdf\nindex bac8a42..edd66d8 100644\n--- a/dissertation.pdf\n+++ b/dissertation.pdf\n@@ -4,17 +4,17 @@ Keywords:\n Author:\n Creator:        LaTeX with hyperref package\n Producer:       pdfTeX-1.40.15\n-CreationDate:   Sat Mar 18 16:03:34 2017\n-ModDate:        Sat Mar 18 16:03:34 2017\n+CreationDate:   Mon Apr  3 00:41:05 2017\n+ModDate:        Mon Apr  3 00:41:05 2017\n Tagged:         no\n UserProperties: no\n Suspects:       no\n Form:           none\n JavaScript:     no\n-Pages:          130\n+Pages:          140\n Encrypted:      no\n Page size:      612 x 792 pts (letter)\n Page rot:       0\n-File size:      13127074 bytes\n+File size:      23775044 bytes\n Optimized:      no\n PDF version:    1.5"
  },
  {
    "objectID": "posts/2017-02-24-healthinf/index.html",
    "href": "posts/2017-02-24-healthinf/index.html",
    "title": "10th International Conference on Health Informatics",
    "section": "",
    "text": "I presented a paper at the 10th International Conference on Health Informatics in Porto Portugal, Feb 21-23, 2017. HealthInf is part of BIOSTEC.\nThe paper I presented was on the use of git submodules as a data security solution for collaborative reproducible reporting. The focus was on using third party hosting services and open source software to minimize financial commitments and relying on institutional firewalls to limit access data submodules.\nTitle: “Collaborative Reproducible Reporting: Git Submodules as a Data Security Solution,”\nAuthors: Peter E. DeWitt and Tellen D. Bennett,\nAbstract: Sensitive data and collaborative projects pose challenges for reproducible computational research. We present a workflow based on literate programming and distributed version control to produce well-documented and dynamic documents collaboratively authored by a team composed of members with varying data access privileges. Data are stored on secure institutional network drives and incorporated into projects using a feature of the Git version control system: submodules. Code to analyze data and write text is managed on public collaborative development environments. This workflow supports collaborative authorship while simultaneously protecting sensitive data. The workflow is designed to be inexpensive and is implemented primarily with a variety of free and open-source software. Work products can be abstracts, manuscripts, posters, slide decks, grant applications, or other documents. This approach is adaptable to teams of varying size in other collaborative situations.\nThe paper itself will be published online as part of the BIOSTEC 2017 Proceedings, at Scitepress.\nThe conference was wonderful. Great keynotes talks. For me, as a bio-statistician, it was great to see other aspects of bio-informatics, sensing, image analysis, process analysis, and translational research.\nI also enjoyed visiting the city or Porto. Great wineries, food, and views on the river front. A must visit.\n\n\n\n\n\n\n\nDom Luis I Bridge"
  },
  {
    "objectID": "posts/2017-01-03-lazyloading-cached-chunks/index.html",
    "href": "posts/2017-01-03-lazyloading-cached-chunks/index.html",
    "title": "(lazy)Loading Cached Chunks into an Interactive R Session",
    "section": "",
    "text": "If you cache code chunks when using knitr to generate reproducible documents then you’ve likely had the issue arrise of needing to load the results of cached chunks into an active interactive R session. The functions lazyload_cache_dir and lazyload_cache_labels have been added to the developmental version of qwraps2 to make loading cached objects into an interactive R session easy.\nLet’s make some trivial cached chunks in a file report.Rmd.\n\n\n      ---\n      title:  \"A Report\"\n      output: html_document\n      ---\n      \n      ```{r, label = \"first-chunk\", cache = TRUE}\n      fit &lt;- lm(mpg ~ wt + hp, data = mtcars)\n      x &lt;- pi\n      ```\n      \n      ```{r, label = \"second-chunk\", cache = TRUE}\n      fit &lt;- lm(mpg ~ wt + hp + am, data = mtcars)\n      xx &lt;- exp(1)\n      ```\n\n\nThe resulting project directory looks like this:\n    .\n    ├── report_cache\n    │   └── html\n    │       ├── first-chunk_bf368425c25f0c3d95cac85aff007ad1.RData\n    │       ├── first-chunk_bf368425c25f0c3d95cac85aff007ad1.rdb\n    │       ├── first-chunk_bf368425c25f0c3d95cac85aff007ad1.rdx\n    │       ├── __packages\n    │       ├── second-chunk_2c7d6b477306be1d4d4ed451f2f1b52a.RData\n    │       ├── second-chunk_2c7d6b477306be1d4d4ed451f2f1b52a.rdb\n    │       └── second-chunk_2c7d6b477306be1d4d4ed451f2f1b52a.rdx\n    ├── report.html\n    └── report.Rmd\nNow, let’s assume you need to come back to this project at a later date. You want to get the objects created in the cached chunks into a new interactive R session.\nIf you use lazyload_cache_dir all the cached chunks will be load.\nrm(list = ls())\nls()\n## character(0)\nqwraps2::lazyload_cache_dir(path = \"report_cache/html\")\n## Lazyloading: report_cache/html/first-chunk\n## Lazyloading: report_cache/html/second-chunk\nls()\n## [1] \"fit\" \"x\"   \"xx\"\nfit\n## \n## Call:\n## lm(formula = mpg ~ wt + hp + am, data = mtcars)\n## \n## Coefficients:\n## (Intercept)           wt           hp           am  \n##    34.00288     -2.87858     -0.03748      2.08371\nx\n## [1] 3.141593\nxx\n## [1] 2.718282\nThis could be a problem. There was a fit created in first-chunk and another object fit created in second-chunk. The fit object in the active workspace is from second-chunk. What if you want the fit from first-chunk? Use lazyload_cache_labels.\nrm(list = ls())\nls()\n## character(0)\nqwraps2::lazyload_cache_labels(\"first-chunk\", path = \"report_cache/html\")\n## Lazyloading report_cache/html/first-chunk_bf368425c25f0c3d95cac85aff007ad1\nls()\n## [1] \"fit\" \"x\"\nfit\n## \n## Call:\n## lm(formula = mpg ~ wt + hp, data = mtcars)\n## \n## Coefficients:\n## (Intercept)           wt           hp  \n##    37.22727     -3.87783     -0.03177\nx\n## [1] 3.141593\nxx\n## Error in eval(expr, envir, enclos): object 'xx' not found\nNow, what if you want fit from first-chunk and xx which was created in second-chunk?\nrm(list = ls())\nls()\n## character(0)\nqwraps2::lazyload_cache_dir(path = \"report_cache/html\")\n## Lazyloading: report_cache/html/first-chunk\n## Lazyloading: report_cache/html/second-chunk\nqwraps2::lazyload_cache_labels(\"first-chunk\", path = \"report_cache/html\")\n## Lazyloading report_cache/html/first-chunk_bf368425c25f0c3d95cac85aff007ad1\nls()\n## [1] \"fit\" \"x\"   \"xx\"\nfit\n## \n## Call:\n## lm(formula = mpg ~ wt + hp, data = mtcars)\n## \n## Coefficients:\n## (Intercept)           wt           hp  \n##    37.22727     -3.87783     -0.03177\nx\n## [1] 3.141593\nxx\n## [1] 2.718282\nA better solution would be to use unique object names.\nLastly, say you only want the fit from first-chunk and no other objects. The optional filter argument of lazyload_cache_labels can be used. The filter argument is passed to lazyLoad. “filter: An optional function which when called on a character vector of object names returns a logical vector: only objects for which this is true will be loaded.”\nrm(list = ls())\nls()\n## character(0)\nqwraps2::lazyload_cache_labels(\"first-chunk\",\n                               path = \"report_cache/html\",\n                               filter = function(x) x == \"fit\")\n## Lazyloading report_cache/html/first-chunk_bf368425c25f0c3d95cac85aff007ad1\nls()\n## [1] \"fit\"\nfit\n## \n## Call:\n## lm(formula = mpg ~ wt + hp, data = mtcars)\n## \n## Coefficients:\n## (Intercept)           wt           hp  \n##    37.22727     -3.87783     -0.03177\nThe lazyload_cache_dir and lazyload_cache_labels functions are currently in the developmental version of qwarps2. You can get the code and install the package from https://github.com/dewittpe/qwraps2. I’m not sure when the next version of qwraps2 will be pushed to CRAN, so the developmental version will have to do for now.\nIf you find any bugs or have suggestions on how to improve the functions please create and issue.\nNote, the behavior of knitr::load_cache is very different from the behavior of qwraps2::lazyload_cache_dir and qwraps2::lazyload_cache_labels. knitr::load_cache is to be used to load cached values within a .Rmd prior to the chunk being called. See the documentation and example 114-load-cache.Rmd from the knitr-examples repository for the use of knitr::load_cache."
  },
  {
    "objectID": "posts/2024-10-01-rebuild/index.html",
    "href": "posts/2024-10-01-rebuild/index.html",
    "title": "Site Rebuild",
    "section": "",
    "text": "Rebuilding the site\nI have let the landing page and blog section of peteredewitt.com languish for years. It is about time that I rebuild this. I’m working on moving away from Jekyll and to a quarto based workflow."
  },
  {
    "objectID": "posts/2018-02-25-installing-dependencies/index.html",
    "href": "posts/2018-02-25-installing-dependencies/index.html",
    "title": "Installing Package Dependencies without external http(s) requests",
    "section": "",
    "text": "Consider you have a server that is running behind a firewall and, for security reasons, cannot make external http(s) requests. Further, you have R running on this server and you need to install a set of packages. The simple approach of\nis not an option since you will have no access to the CRAN repository.\nAnother option would be to download the source files (.tar.gz files) form CRAN or BioConductor, transfer those files to the sever via FTP, and then install the packages via\nThis approach will work well, with one big exception, the dependencies of the package may not be on the server. How do you get the source files for all the dependencies of your package? What about the dependencies of the dependencies, and the dependencies of the dependencies of the dependencies? Simply, how do you install R packages on a machine that is not allowed to make external http(s) requests?\nHere is how I approached this problem. On my local machine, a machine with internet access, I ran a script (a script that will be shown and explained in detail below) which will download all the dependencies and dependencies of dependencies, etc., from both CRAN and BioConductor, and generate a makefile to install the packages in the correct order, i.e., in an order such that the dependencies are met.\nWhen the script finished, the source files and the makefile can be transfered to the server without external http(s) request authority. Running the makefile will install the packages, and is an easy way to track and report install errors.\nWe need to define what constitutes a dependency. In a package DESCRIPTION file packages listed under the field Depends, Imports, and LinkingTo are what we will consider dependencies. Suggests and Enhances are omitted as they are not needed for the package to work."
  },
  {
    "objectID": "posts/2018-02-25-installing-dependencies/index.html#build-dependencies",
    "href": "posts/2018-02-25-installing-dependencies/index.html#build-dependencies",
    "title": "Installing Package Dependencies without external http(s) requests",
    "section": "Build Dependencies",
    "text": "Build Dependencies\nAn R script build-dep-list.R has been written and is expected to be evaluated from the command line via\nRscript --vanilla build-dep-list.R [pkg1] [pkg2] [...] [pkgn]\nWhere pkg1 is the name of the first known package to download, pkg2 the second known package to download, …, and pkgn the nth package to download. The script will download all the dependencies for pkg1, ..., pkgn, and the dependencies of the dependencies, and so on. The script will also generate a makefile to help with the installation of the packages, aiming to get the order of the installs so that the install of pkg1, ..., pkgn will not error.\nThe full script can be found on my github page. The script will be broken up into pieces here with additional detail and explanation.\nWhen I develop scripts that I expect to evaluated in the terminal, I will start the script with a check of interactive(). If in an interactive session we’ll have set variables to values needed for testing and development, and if not in an interactive session we’ll use the command line arguments to define the value of the variables. This could also be edited so that the expected evaluation would be done in an interactive session. For then work we will have the character vector OUR_PACKAGES to store the names of the packages we want/need to install.\nif (interactive()) {\n  OUR_PACKAGES &lt;- c(\"graph\", \"gRbase\", \"gRain\", \"jsonlite\", \"plotly\", \"SHELF\",\n                    \"rjson\", \"svglite\", \"magrittr\")\n} else {\n  OUR_PACKAGES &lt;- commandArgs(trailingOnly = TRUE)\n}\nWe also need to define the repositories which we will query for the packages. We’ll use RStudio’s CRAN mirror and the repository for BioConductor.\n# Repositories to look for packages\nCRAN &lt;- \"https://cran.rstudio.com/\"\nBIOC &lt;- \"https://bioconductor.org/packages/release/bioc/\"\nNow, let’s look into the packages. Packages are classified into three priority classes, “base”, “recommended”, and “NA”. The “base” packages are standard an R installation, and the ‘recommended’ are in any standard installation of R. All other packages have Priority == NA.\nipkgs &lt;- utils::installed.packages()\nipkgs[ipkgs[, \"Priority\"] %in% \"base\", \"Package\"]\n##        base    compiler    datasets    graphics   grDevices        grid\n##      \"base\"  \"compiler\"  \"datasets\"  \"graphics\" \"grDevices\"      \"grid\"\n##     methods    parallel     splines       stats      stats4       tcltk\n##   \"methods\"  \"parallel\"   \"splines\"     \"stats\"    \"stats4\"     \"tcltk\"\n##       tools       utils\n##     \"tools\"     \"utils\"\nipkgs[ipkgs[, \"Priority\"] %in% \"recommended\", \"Package\"]\n##         boot        class      cluster    codetools      foreign\n##       \"boot\"      \"class\"    \"cluster\"  \"codetools\"    \"foreign\"\n##   KernSmooth      lattice         MASS       Matrix         mgcv\n## \"KernSmooth\"    \"lattice\"       \"MASS\"     \"Matrix\"       \"mgcv\"\n##         nnet        rpart      spatial     survival\n##       \"nnet\"      \"rpart\"    \"spatial\"   \"survival\"\nSome packages will have dependencies on the “base” and/or “recommended” packages. We will need to know these packages and omit them form the packages we will need to download and install.\nbase_pkgs &lt;-\n  unname(utils::installed.packages()[utils::installed.packages()[, \"Priority\"] %in% c(\"base\", \"recommended\"), \"Package\"])\nNext step, get a list of the available packages from CRAN and BioConductor. The return from available.packages is a matrix with all the information we will need about the packages.\navailable_pkgs &lt;- available.packages(repos = c(CRAN, BIOC))\nstr(available_pkgs)\n##  chr [1:13659, 1:17] \"A3\" \"abbyyR\" \"abc\" \"abc.data\" \"ABC.RAP\" ...\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : chr [1:13659] \"A3\" \"abbyyR\" \"abc\" \"abc.data\" ...\n##   ..$ : chr [1:17] \"Package\" \"Version\" \"Priority\" \"Depends\" ...\n\navailable_pkgs[available_pkgs[, \"Package\"] %in% OUR_PACKAGES,\n               c(\"Package\", \"Version\", \"Depends\", \"Imports\", \"LinkingTo\",\n                 \"Repository\")]\n##          Package    Version\n## gRain    \"gRain\"    \"1.3-0\"\n## gRbase   \"gRbase\"   \"1.8-3\"\n## jsonlite \"jsonlite\" \"1.5\"\n## magrittr \"magrittr\" \"1.5\"\n## plotly   \"plotly\"   \"4.7.1\"\n## rjson    \"rjson\"    \"0.2.15\"\n## SHELF    \"SHELF\"    \"1.3.0\"\n## svglite  \"svglite\"  \"1.2.1\"\n## graph    \"graph\"    \"1.56.0\"\n##          Depends\n## gRain    \"R (&gt;= 3.0.2), methods, gRbase (&gt;= 1.7-2)\"\n## gRbase   \"R (&gt;= 3.0.2), methods\"\n## jsonlite \"methods\"\n## magrittr NA\n## plotly   \"R (&gt;= 3.2.0), ggplot2 (&gt;= 2.2.1)\"\n## rjson    \"R (&gt;= 3.1.0)\"\n## SHELF    \"R (&gt;= 3.3.1)\"\n## svglite  \"R (&gt;= 3.0.0)\"\n## graph    \"R (&gt;= 2.10), methods, BiocGenerics (&gt;= 0.13.11)\"\n##          Imports\n## gRain    \"igraph, graph, magrittr, functional, Rcpp (&gt;= 0.11.1)\"\n## gRbase   \"graph, igraph, magrittr, Matrix, RBGL, Rcpp (&gt;= 0.11.1)\"\n## jsonlite NA\n## magrittr NA\n## plotly   \"tools, scales, httr, jsonlite, magrittr, digest, viridisLite,\\nbase64enc, htmltools, htmlwidgets (&gt;= 0.9), tidyr, hexbin,\\nRColorBrewer, dplyr, tibble, lazyeval (&gt;= 0.2.0), crosstalk,\\npurrr, data.table\"\n## rjson    NA\n## SHELF    \"ggplot2, grid, shiny, stats, graphics, tidyr, MASS, ggExtra\"\n## svglite  \"Rcpp, gdtools (&gt;= 0.1.6)\"\n## graph    \"stats, stats4, utils\"\n##          LinkingTo\n## gRain    \"Rcpp (&gt;= 0.11.1), RcppArmadillo, RcppEigen, gRbase (&gt;=\\n1.8-0)\"\n## gRbase   \"Rcpp (&gt;= 0.11.1), RcppArmadillo, RcppEigen\"\n## jsonlite NA\n## magrittr NA\n## plotly   NA\n## rjson    NA\n## SHELF    NA\n## svglite  \"Rcpp, gdtools, BH\"\n## graph    NA\n##          Repository\n## gRain    \"https://cran.rstudio.com/src/contrib\"\n## gRbase   \"https://cran.rstudio.com/src/contrib\"\n## jsonlite \"https://cran.rstudio.com/src/contrib\"\n## magrittr \"https://cran.rstudio.com/src/contrib\"\n## plotly   \"https://cran.rstudio.com/src/contrib\"\n## rjson    \"https://cran.rstudio.com/src/contrib\"\n## SHELF    \"https://cran.rstudio.com/src/contrib\"\n## svglite  \"https://cran.rstudio.com/src/contrib\"\n## graph    \"https://bioconductor.org/packages/release/bioc/src/contrib\"\nIn this example we see that the packages listed in OUR_PACKAGES except the graph package can be downloaded from CRAN. graph and at least one dependencies, BiocGenerics will need to be downloaded from BioConductor.\nThe next step in building the list of dependencies and a script for installing them is done in the following while loop. We start with a character vector pkgs_to_download which is initially equivalent to OUR_PACKAGES. We will iterate through this vector, appending the dependencies in order. Use the tools::package_dependencies function to generate a list of the packages dependencies, and dependencies of dependencies, and so on.\nIn the while loop we get a list of the dependencies for a package, stored in the deps object. We will omit any of the base and recommended packages from the deps object and then append deps to the pkgs_to_download vector in the position immediately to the right of the current package being looked up. When the indexer i is incremented, the next package to be considered will be the first dependency. This process continues until all the dependencies have been explored. Lastly, we reverse the order of the elements of pkgs_to_download so that we have the packages listed in a useful install order, i.e., pkgs_to_download[1] should be installed before pkgs_to_download[2], etc. After reversing the order of the elements of pkgs_to_download we look only at the unique elements. By default, the first occurrence of an element will be keep and the repeated elements will be omitted. By reversing the order then taking the unique values, the deepest level of dependency will be retained for a specific package.\npkgs_to_download &lt;- OUR_PACKAGES\ni &lt;- 1L\nwhile(i &lt;= length(pkgs_to_download)) {\n  deps &lt;-\n    unlist(tools::package_dependencies(packages = pkgs_to_download[i],\n                                       which = c(\"Depends\", \"Imports\", \"LinkingTo\"),\n                                       db = available_pkgs,\n                                       recursive = FALSE),\n           use.names = FALSE)\n  deps &lt;- deps[!(deps %in% base_pkgs)]\n  pkgs_to_download &lt;- append(pkgs_to_download, deps, i)\n  i &lt;- i + 1L\n}\npkgs_to_download &lt;- unique(rev(pkgs_to_download))\nIf you are having a difficult time envisioning what the above does, let’s look at and example for the dplyr package. In this example we’ll print out the list of dependencies at each step through the while loop. Note that packages such as Rcpp will be assessed multiple times, but the final list will only have Rcpp listed once.\ndplyr_dependencies &lt;- \"dplyr\"\ni &lt;- 1L\nwhile(i &lt;= length(dplyr_dependencies)) {\n\n  cat(\"\\ni =\", i, \"\\nLooking up dependencies for\", dplyr_dependencies[i], \"\\n\")\n  deps &lt;-\n    unlist(tools::package_dependencies(packages = dplyr_dependencies[i],\n                                       which = c(\"Depends\", \"Imports\", \"LinkingTo\"),\n                                       db = available_pkgs,\n                                       recursive = FALSE),\n           use.names = FALSE)\n  deps &lt;- deps[!(deps %in% base_pkgs)]\n  dplyr_dependencies &lt;- append(dplyr_dependencies, deps, i)\n\n  cat(dplyr_dependencies[i], \"dependencies:\", paste(deps, collapse = \", \"),\n      \"\\ndplyr_dependencies =\", paste(dplyr_dependencies, collapse = \", \"), \"\\n\")\n\n  i &lt;- i + 1L\n}\n##\n## i = 1\n## Looking up dependencies for dplyr\n## dplyr dependencies: assertthat, bindrcpp, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 2\n## Looking up dependencies for assertthat\n## assertthat dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 3\n## Looking up dependencies for bindrcpp\n## bindrcpp dependencies: Rcpp, bindr, plogr\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 4\n## Looking up dependencies for Rcpp\n## Rcpp dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 5\n## Looking up dependencies for bindr\n## bindr dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 6\n## Looking up dependencies for plogr\n## plogr dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 7\n## Looking up dependencies for glue\n## glue dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 8\n## Looking up dependencies for magrittr\n## magrittr dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 9\n## Looking up dependencies for pkgconfig\n## pkgconfig dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 10\n## Looking up dependencies for rlang\n## rlang dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 11\n## Looking up dependencies for R6\n## R6 dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 12\n## Looking up dependencies for Rcpp\n## Rcpp dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, BH, plogr\n##\n## i = 13\n## Looking up dependencies for tibble\n## tibble dependencies: cli, crayon, pillar, rlang\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, crayon, pillar, rlang, BH, plogr\n##\n## i = 14\n## Looking up dependencies for cli\n## cli dependencies: assertthat, crayon\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, rlang, BH, plogr\n##\n## i = 15\n## Looking up dependencies for assertthat\n## assertthat dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, rlang, BH, plogr\n##\n## i = 16\n## Looking up dependencies for crayon\n## crayon dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, rlang, BH, plogr\n##\n## i = 17\n## Looking up dependencies for crayon\n## crayon dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, rlang, BH, plogr\n##\n## i = 18\n## Looking up dependencies for pillar\n## pillar dependencies: cli, crayon, rlang, utf8\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 19\n## Looking up dependencies for cli\n## cli dependencies: assertthat, crayon\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 20\n## Looking up dependencies for assertthat\n## assertthat dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 21\n## Looking up dependencies for crayon\n## crayon dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 22\n## Looking up dependencies for crayon\n## crayon dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 23\n## Looking up dependencies for rlang\n## rlang dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 24\n## Looking up dependencies for utf8\n## utf8 dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 25\n## Looking up dependencies for rlang\n## rlang dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 26\n## Looking up dependencies for BH\n## BH dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\n##\n## i = 27\n## Looking up dependencies for plogr\n## plogr dependencies:\n## dplyr_dependencies = dplyr, assertthat, bindrcpp, Rcpp, bindr, plogr, glue, magrittr, pkgconfig, rlang, R6, Rcpp, tibble, cli, assertthat, crayon, crayon, pillar, cli, assertthat, crayon, crayon, rlang, utf8, rlang, BH, plogr\ndplyr_dependencies &lt;- unique(rev(dplyr_dependencies))\ndplyr_dependencies\n##  [1] \"plogr\"      \"BH\"         \"rlang\"      \"utf8\"       \"crayon\"\n##  [6] \"assertthat\" \"cli\"        \"pillar\"     \"tibble\"     \"Rcpp\"\n## [11] \"R6\"         \"pkgconfig\"  \"magrittr\"   \"glue\"       \"bindr\"\n## [16] \"bindrcpp\"   \"dplyr\"\nNow that we have pkgs_to_download, a character vector of package names that we need to download, we can use the download.packages function to do so. The object dwnld_pkgs is a 2 column matrix with the name and file path to the source file for each package.\n# Download the needed packages into the pkg-source-files directory\nunlink(\"pkg-source-files/*\")\ndir.create(\"pkg-source-files/\", showWarnings = FALSE)\n\ndwnld_pkgs &lt;-\n  download.packages(pkgs = pkgs_to_download,\n                    destdir = \"pkg-source-files\",\n                    repos = c(CRAN, BIOC),\n                    type = \"source\")\n\nhead(dwnld_pkgs)\nThe last step for the script to run on a machine with external http(s) request authority, is to build a makefile to install all the needed packages. I prefer the makefile over a bash script because of the default error handling that a make provided compared to a bash script.\ncat(\"all:\\n\",\n    paste0(\"\\tR CMD INSTALL \", dwnld_pkgs[, 2], \"\\n\"),\n    sep = \"\",\n    file = \"makefile\")\nFor this example, the first several lines of the makefile are:\nall:\n  R CMD INSTALL pkg-source-files/magrittr_1.5.tar.gz\n  R CMD INSTALL pkg-source-files/BH_1.66.0-1.tar.gz\n  R CMD INSTALL pkg-source-files/withr_2.1.1.tar.gz\n  R CMD INSTALL pkg-source-files/Rcpp_0.12.15.tar.gz\n  R CMD INSTALL pkg-source-files/gdtools_0.1.6.tar.gz\n  R CMD INSTALL pkg-source-files/svglite_1.2.1.tar.gz\nNote that magrittr is the last package in the OUR_PACKAGES object and has no dependencies, thus is the first package installed. The svglite package is the second to last package in OUR_PACKAGES and it will be installed after the dependencies BH, withr, Rcpp, and gdtools are installed."
  },
  {
    "objectID": "posts/2018-02-25-installing-dependencies/index.html#installing-on-the-remote-machine",
    "href": "posts/2018-02-25-installing-dependencies/index.html#installing-on-the-remote-machine",
    "title": "Installing Package Dependencies without external http(s) requests",
    "section": "Installing on the Remote Machine",
    "text": "Installing on the Remote Machine\nNow that the source files have been downloaded and the makefile generated, move the pkg-source-files directory and the makefile to the remote machine and run the makefile. If the makefile fails, there might be some system dependencies that need to be updated."
  },
  {
    "objectID": "posts/2018-02-25-installing-dependencies/index.html#download-the-script-andor-contribute",
    "href": "posts/2018-02-25-installing-dependencies/index.html#download-the-script-andor-contribute",
    "title": "Installing Package Dependencies without external http(s) requests",
    "section": "Download the script and/or contribute",
    "text": "Download the script and/or contribute\nThe build-dependency-list.R file can be found on my github page."
  },
  {
    "objectID": "posts/2016-12-19-summary-table/index.html",
    "href": "posts/2016-12-19-summary-table/index.html",
    "title": "Baseline Characteristics Tables with qwraps2",
    "section": "",
    "text": "Almost every biomedical research paper requires a “Table 1: baseline patient characteristics.” Many developers have published tools to help streamline the construction of such tables. The qwraps2::summary_table function is my contribution to the toolbox.\nI have constructed hundreds of Table 1s while working as a biostatistics consultant in a biostatistics department. I’ve also constructed many helper functions to try and streamline the production of such tables. However, every project, every data set, every lead author, every target journal, etc., will present slightly different requirements for the contents and formating of the tables. As such, functions which tried to “do it all”, required constant modification to provide the needed output from each nuanced project.\nThe tableone package does a lot of good things, and is a great tool for quickly building the baseline summary tables. What my experiences has taught me is that each row group, or even each row, might require some specific formating. A function that treats all continuous variables one way and all categorical variables another way, may work for many cases but not all.\nThe approach to building the tables I’ve taken now is explicitly define the summary statistics I want for each variable in the data set, the formatting for the summary statistics, and in a way that is easy to work with one or more grouping variables.\nThe function summary_table within my qwraps2 package is the tool I and a few colleagues have started to rely on for building baseline patient characteristic tables. (qwraps2, “quick wraps 2”, is a package of formatting functions I’ve found useful for formating results and generating some graphics when authoring .Rmd and .Rnw files.)\nLoad and attach the qwraps2 namespace. We’ll set the qwraps2_markup option to markdown. If this option is not set, qwraps2 uses get0ption(qwraps2_markup, \"latex\") as the default markup language.\nlibrary(qwraps2)\noptions(qwraps2_markup = 'markdown') # default is latex\n\nWe’ll use the mtcars dataset for our examples. Let’s report several summary statistics for miles per gallon, number of cylinders, and weight of the vehicles. The following summary is provided to illustrate the functions and thus will include some summaries that would not be used in a publication.\nThe data summary we want will be:\n\nMiles Per Gallon\n\nmin\nmean (sd)\nmedian (iqr)\nmax\n\nCylinders\n\nmean\nn (%) of four cylinders engines\nn (%) of six cylinders engines\nn (%) of eight cylinders engines\n\nWeight\n\nrange\n\n\nFor cylinders we’ll report several things, the mean number of cylinders, and the count (%) of 4, 6, and 8 cylinders cars. In a publication we would likely not report such a summary, treating cylinders as both a continuous and categorical value. However, doing so here helps to illustrate the flexibility of the summary_table method.\nOutlining the wanted summary statistics above as a list-of-lists helps to explain the construction of the summary object constructed below. The summary_table method takes two arguments,\n\n.data, a data.frame or a grouped_df object, and\nsummaries a list-of-lists of right hand sided formulae defining the summary statistics.\n\nThe construction of the summary table is achieved via dplyr::summarize_.\nThe mtcar_summaries object constructed below, defines each needed row of the summary table via a formula. I’ve included the qwraps2 namespace for clarity.\nmtcar_summaries &lt;-\n  list(\"Miles Per Gallon\" =\n       list(\"min:\"         = ~ min(mpg),\n            \"mean (sd)\"    = ~ qwraps2::mean_sd(mpg, denote_sd = \"paren\"),\n            \"median (IQR)\" = ~ qwraps2::median_iqr(mpg),\n            \"max:\"         = ~ max(mpg)),\n       \"Cylinders:\" = \n       list(\"mean\"             = ~ mean(cyl),\n            \"mean (formatted)\" = ~ qwraps2::frmt(mean(cyl)),\n            \"4 cyl, n (%)\"     = ~ qwraps2::n_perc0(cyl == 4),\n            \"6 cyl, n (%)\"     = ~ qwraps2::n_perc0(cyl == 6),\n            \"8 cyl, n (%)\"     = ~ qwraps2::n_perc0(cyl == 8)),\n       \"Weight\" =\n       list(\"Range\" = ~ paste(range(wt), collapse = \", \"))\n       )\n\nThe table is constructed and printed with ease:\nsummary_table(mtcars, mtcar_summaries)\n## \n## \n## |                              |mtcars (N = 32)      |\n## |:-----------------------------|:--------------------|\n## |**Miles Per Gallon**          |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; min:             |10.4                 |\n## |&nbsp;&nbsp; mean (sd)        |20.09 (6.03)         |\n## |&nbsp;&nbsp; median (IQR)     |19.20 (15.43, 22.80) |\n## |&nbsp;&nbsp; max:             |33.9                 |\n## |**Cylinders:**                |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; mean             |6.1875               |\n## |&nbsp;&nbsp; mean (formatted) |6.19                 |\n## |&nbsp;&nbsp; 4 cyl, n (%)     |11 (34)              |\n## |&nbsp;&nbsp; 6 cyl, n (%)     |7 (22)               |\n## |&nbsp;&nbsp; 8 cyl, n (%)     |14 (44)              |\n## |**Weight**                    |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; Range            |1.513, 5.424         |\nThe markdown output, rendered as html is:\n\n\n\n\nmtcars (N = 32)\n\n\n\n\nMiles Per Gallon\n  \n\n\n   min:\n10.4\n\n\n   mean (sd)\n20.09 (6.03)\n\n\n   median (IQR)\n19.20 (15.43, 22.80)\n\n\n   max:\n33.9\n\n\nCylinders:\n  \n\n\n   mean\n6.1875\n\n\n   mean (formatted)\n6.19\n\n\n   4 cyl, n (%)\n11 (34)\n\n\n   6 cyl, n (%)\n7 (22)\n\n\n   8 cyl, n (%)\n14 (44)\n\n\nWeight\n  \n\n\n   Range\n1.513, 5.424\n\n\n\n\nExtending the table to show the same summary by a grouping variable, we’ll use am (Transmission: 0 = automatic, 1 = manual), is done as follows:\nsummary_table(dplyr::group_by(mtcars, am), mtcar_summaries)\n## \n## \n## |                              |am: 0 (N = 19)       |am: 1 (N = 13)       |\n## |:-----------------------------|:--------------------|:--------------------|\n## |**Miles Per Gallon**          |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; min:             |10.4                 |15.0                 |\n## |&nbsp;&nbsp; mean (sd)        |17.15 (3.83)         |24.39 (6.17)         |\n## |&nbsp;&nbsp; median (IQR)     |17.30 (14.95, 19.20) |22.80 (21.00, 30.40) |\n## |&nbsp;&nbsp; max:             |24.4                 |33.9                 |\n## |**Cylinders:**                |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; mean             |6.947368             |5.076923             |\n## |&nbsp;&nbsp; mean (formatted) |6.95                 |5.08                 |\n## |&nbsp;&nbsp; 4 cyl, n (%)     |3 (16)               |8 (62)               |\n## |&nbsp;&nbsp; 6 cyl, n (%)     |4 (21)               |3 (23)               |\n## |&nbsp;&nbsp; 8 cyl, n (%)     |12 (63)              |2 (15)               |\n## |**Weight**                    |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; Weight           |2.465, 5.424         |1.513, 3.57          |\n\n\n\n\n\n\n\n\n\nam: 0 (N = 19)\nam: 1 (N = 13)\n\n\n\n\nMiles Per Gallon\n  \n  \n\n\n   min:\n10.4\n15.0\n\n\n   mean (sd)\n17.15 (3.83)\n24.39 (6.17)\n\n\n   median (IQR)\n17.30 (14.95, 19.20)\n22.80 (21.00, 30.40)\n\n\n   max:\n24.4\n33.9\n\n\nCylinders:\n  \n  \n\n\n   mean\n6.947368\n5.076923\n\n\n   mean (formatted)\n6.95\n5.08\n\n\n   4 cyl, n (%)\n3 (16)\n8 (62)\n\n\n   6 cyl, n (%)\n4 (21)\n3 (23)\n\n\n   8 cyl, n (%)\n12 (63)\n2 (15)\n\n\nWeight\n  \n  \n\n\n   Weight\n2.465, 5.424\n1.513, 3.57\n\n\n\n\nAnd lastly, building one table with a column for the whole data set and columns for each transmission type is:\ncbind(summary_table(mtcars, mtcar_summaries),\n      summary_table(dplyr::group_by(mtcars, am), mtcar_summaries))\n## \n## \n## |                              |mtcars (N = 32)      |am: 0 (N = 19)       |am: 1 (N = 13)       |\n## |:-----------------------------|:--------------------|:--------------------|:--------------------|\n## |**Miles Per Gallon**          |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; min:             |10.4                 |10.4                 |15.0                 |\n## |&nbsp;&nbsp; mean (sd)        |20.09 (6.03)         |17.15 (3.83)         |24.39 (6.17)         |\n## |&nbsp;&nbsp; median (IQR)     |19.20 (15.43, 22.80) |17.30 (14.95, 19.20) |22.80 (21.00, 30.40) |\n## |&nbsp;&nbsp; max:             |33.9                 |24.4                 |33.9                 |\n## |**Cylinders:**                |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; mean             |6.1875               |6.947368             |5.076923             |\n## |&nbsp;&nbsp; mean (formatted) |6.19                 |6.95                 |5.08                 |\n## |&nbsp;&nbsp; 4 cyl, n (%)     |11 (34)              |3 (16)               |8 (62)               |\n## |&nbsp;&nbsp; 6 cyl, n (%)     |7 (22)               |4 (21)               |3 (23)               |\n## |&nbsp;&nbsp; 8 cyl, n (%)     |14 (44)              |12 (63)              |2 (15)               |\n## |**Weight**                    |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; Range            |1.513, 5.424         |2.465, 5.424         |1.513, 3.57          |\n\n\n\n\n\n\n\n\n\n\nmtcars (N = 32)\nam: 0 (N = 19)\nam: 1 (N = 13)\n\n\n\n\nMiles Per Gallon\n  \n  \n  \n\n\n   min:\n10.4\n10.4\n15.0\n\n\n   mean (sd)\n20.09 (6.03)\n17.15 (3.83)\n24.39 (6.17)\n\n\n   median (IQR)\n19.20 (15.43, 22.80)\n17.30 (14.95, 19.20)\n22.80 (21.00, 30.40)\n\n\n   max:\n33.9\n24.4\n33.9\n\n\nCylinders:\n  \n  \n  \n\n\n   mean\n6.1875\n6.947368\n5.076923\n\n\n   mean (formatted)\n6.19\n6.95\n5.08\n\n\n   4 cyl, n (%)\n11 (34)\n3 (16)\n8 (62)\n\n\n   6 cyl, n (%)\n7 (22)\n4 (21)\n3 (23)\n\n\n   8 cyl, n (%)\n14 (44)\n12 (63)\n2 (15)\n\n\nWeight\n  \n  \n  \n\n\n   Range\n1.513, 5.424\n2.465, 5.424\n1.513, 3.57\n\n\n\n\nUsing dplry::group_by will allow you to build the table with more than one grouping variable. For example:\ncbind(summary_table(mtcars, mtcar_summaries),\n      summary_table(dplyr::group_by(mtcars, am, vs), mtcar_summaries))\n## \n## \n## |                              |mtcars (N = 32)      |am: 0 vs: 0 (N = 12) |am: 0 vs: 1 (N = 7)  |am: 1 vs: 0 (N = 6)  |am: 1 vs: 1 (N = 7)  |\n## |:-----------------------------|:--------------------|:--------------------|:--------------------|:--------------------|:--------------------|\n## |**Miles Per Gallon**          |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; min:             |10.4                 |10.4                 |17.8                 |15.0                 |21.4                 |\n## |&nbsp;&nbsp; mean (sd)        |20.09 (6.03)         |15.05 (2.77)         |20.74 (2.47)         |19.75 (4.01)         |28.37 (4.76)         |\n## |&nbsp;&nbsp; median (IQR)     |19.20 (15.43, 22.80) |15.20 (14.05, 16.62) |21.40 (18.65, 22.15) |20.35 (16.78, 21.00) |30.40 (25.05, 31.40) |\n## |&nbsp;&nbsp; max:             |33.9                 |19.2                 |24.4                 |26.0                 |33.9                 |\n## |**Cylinders:**                |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; mean             |6.1875               |8.000000             |5.142857             |6.333333             |4.000000             |\n## |&nbsp;&nbsp; mean (formatted) |6.19                 |8.00                 |5.14                 |6.33                 |4.00                 |\n## |&nbsp;&nbsp; 4 cyl, n (%)     |11 (34)              |0 (0)                |3 (43)               |1 (17)               |7 (100)              |\n## |&nbsp;&nbsp; 6 cyl, n (%)     |7 (22)               |0 (0)                |4 (57)               |3 (50)               |0 (0)                |\n## |&nbsp;&nbsp; 8 cyl, n (%)     |14 (44)              |12 (100)             |0 (0)                |2 (33)               |0 (0)                |\n## |**Weight**                    |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |&nbsp;&nbsp;         |\n## |&nbsp;&nbsp; Range            |1.513, 5.424         |3.435, 5.424         |2.465, 3.46          |2.14, 3.57           |1.513, 2.78          |\n\n\n\n\n\n\n\n\n\n\n\n\nmtcars (N = 32)\nam: 0 vs: 0 (N = 12)\nam: 0 vs: 1 (N = 7)\nam: 1 vs: 0 (N = 6)\nam: 1 vs: 1 (N = 7)\n\n\n\n\nMiles Per Gallon\n  \n  \n  \n  \n  \n\n\n   min:\n10.4\n10.4\n17.8\n15.0\n21.4\n\n\n   mean (sd)\n20.09 (6.03)\n15.05 (2.77)\n20.74 (2.47)\n19.75 (4.01)\n28.37 (4.76)\n\n\n   median (IQR)\n19.20 (15.43, 22.80)\n15.20 (14.05, 16.62)\n21.40 (18.65, 22.15)\n20.35 (16.78, 21.00)\n30.40 (25.05, 31.40)\n\n\n   max:\n33.9\n19.2\n24.4\n26.0\n33.9\n\n\nCylinders:\n  \n  \n  \n  \n  \n\n\n   mean\n6.1875\n8.000000\n5.142857\n6.333333\n4.000000\n\n\n   mean (formatted)\n6.19\n8.00\n5.14\n6.33\n4.00\n\n\n   4 cyl, n (%)\n11 (34)\n0 (0)\n3 (43)\n1 (17)\n7 (100)\n\n\n   6 cyl, n (%)\n7 (22)\n0 (0)\n4 (57)\n3 (50)\n0 (0)\n\n\n   8 cyl, n (%)\n14 (44)\n12 (100)\n0 (0)\n2 (33)\n0 (0)\n\n\nWeight\n  \n  \n  \n  \n  \n\n\n   Range\n1.513, 5.424\n3.435, 5.424\n2.465, 3.46\n2.14, 3.57\n1.513, 2.78\n\n\n\n\nThe formatting of the output is controlled by the qwraps2:::print.qwraps2_summary_table and qwraps2::qable functions.\nargs(qwraps2:::print.qwraps2_summary_table)\n## function (x, rgroup = attr(x, \"rgroups\"), rnames = rownames(x), \n##     cnames = colnames(x), ...) \n## NULL\nargs(qwraps2::qable)\n## function (x, rtitle, rgroup, rnames = rownames(x), cnames = colnames(x), \n##     markup = getOption(\"qwraps2_markup\", \"latex\"), ...) \n## NULL\nThe print method for qwraps2_summary_table objects calls qable which is a wrapper around knitr::kable. The row groups,rgroup, row names rnames, and column names, cnames, are explicitly set in the print.qwraps2_summary_table method. The ... passes additional arguments to qwraps2::qable which can then continue to pass to knitr::kable.\nA quick example of modifying a table:\nby_am &lt;- summary_table(dplyr::group_by(mtcars, am), mtcar_summaries)\n\nprint(by_am, \n      cnames = c(\"_Manual_\", \"_Automatic_\"),\n      rtitle = \"Vehicle Characteristics\", \n      align = \"lcc\")\n## \n## \n## |Vehicle Characteristics       |       _Manual_       |     _Automatic_      |\n## |:-----------------------------|:--------------------:|:--------------------:|\n## |**Miles Per Gallon**          |     &nbsp;&nbsp;     |     &nbsp;&nbsp;     |\n## |&nbsp;&nbsp; min:             |         10.4         |         15.0         |\n## |&nbsp;&nbsp; mean (sd)        |     17.15 (3.83)     |     24.39 (6.17)     |\n## |&nbsp;&nbsp; median (IQR)     | 17.30 (14.95, 19.20) | 22.80 (21.00, 30.40) |\n## |&nbsp;&nbsp; max:             |         24.4         |         33.9         |\n## |**Cylinders:**                |     &nbsp;&nbsp;     |     &nbsp;&nbsp;     |\n## |&nbsp;&nbsp; mean             |       6.947368       |       5.076923       |\n## |&nbsp;&nbsp; mean (formatted) |         6.95         |         5.08         |\n## |&nbsp;&nbsp; 4 cyl, n (%)     |        3 (16)        |        8 (62)        |\n## |&nbsp;&nbsp; 6 cyl, n (%)     |        4 (21)        |        3 (23)        |\n## |&nbsp;&nbsp; 8 cyl, n (%)     |       12 (63)        |        2 (15)        |\n## |**Weight**                    |     &nbsp;&nbsp;     |     &nbsp;&nbsp;     |\n## |&nbsp;&nbsp; Weight           |     2.465, 5.424     |     1.513, 3.57      |\n\n\n\n\n\n\n\n\nVehicle Characteristics\nManual\nAutomatic\n\n\n\n\nMiles Per Gallon\n  \n  \n\n\n   min:\n10.4\n15.0\n\n\n   mean (sd)\n17.15 (3.83)\n24.39 (6.17)\n\n\n   median (IQR)\n17.30 (14.95, 19.20)\n22.80 (21.00, 30.40)\n\n\n   max:\n24.4\n33.9\n\n\nCylinders:\n  \n  \n\n\n   mean\n6.947368\n5.076923\n\n\n   mean (formatted)\n6.95\n5.08\n\n\n   4 cyl, n (%)\n3 (16)\n8 (62)\n\n\n   6 cyl, n (%)\n4 (21)\n3 (23)\n\n\n   8 cyl, n (%)\n12 (63)\n2 (15)\n\n\nWeight\n  \n  \n\n\n   Weight\n2.465, 5.424\n1.513, 3.57\n\n\n\n\nI hope that some readers will find this approach to building summary tables to be useful. If you find bugs or have suggestions on how to extend and improve this tool please create an issue on github."
  },
  {
    "objectID": "posts/2016-10-26-set-up/index.html",
    "href": "posts/2016-10-26-set-up/index.html",
    "title": "dewittpe.github.io Set up",
    "section": "",
    "text": "I’ve spent the last several hours of yesterday this early morning putting the initial dewittpe.github.io page together. I hope you, the wandering web surfer, or the unfortunately sentient web spider, enjoy what you have found."
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "",
    "text": "Two major publications in the Journal of the American Medical Association (JAMA) have been released International Consensus Criteria for Pediatric Sepsis and Septic Shock and Development and Validation of the Phoenix Criteria for Pediatric Sepsis and Septic Shock detailing the efforts of an international team of researchers to modernize and improve on the diagnostic criteria for pediatric sepsis. I am fortunate to have been part of this team and co-second author on the “Development” manuscript, and co-author on the consensus manuscript.\nThese publications report on the development and consensus for the Phoenix Sepsis Score as the new diagnostic criteria for pediatric sepsis and septic shock. The publication coincides with the criteria being presented at the 2024 Critical Care Congress of the Society for Critical Care Medicine (SCCM)\nThe Phoenix criteria will be utilized for diagnosing pediatric sepsis and septic shock in children all over the world."
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html#the-big-picture",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html#the-big-picture",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "The big picture",
    "text": "The big picture\nGlobally, pediatric sepsis is responsible for an estimated 3.3 million deaths per year.\nPrior to 2016, the diagnostic criteria for both adult and pediatric sepsis was based on an inflammatory response. That criteria had low sensitivity and was difficult to implement across differently resourced medical facilities. In 2016 the diagnostic criteria for adult sepsis was redefined based on life-threatening organ dysfunction. The publication of the Phoenix criteria in 2024 brings the diagnostic criteria for pediatric sepsis in-line with the adult sepsis definition as both are now based on life-threatening organ dysfunction. Additionally, the Phoenix criteria was developed to be useful in high- and low-resourced environments, that is, the criteria is applicable across the globe, in major metropolitan area research hospitals and rural medical clinics."
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html#how-was-the-phoenix-criteria-developed",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html#how-was-the-phoenix-criteria-developed",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "How was the Phoenix Criteria Developed",
    "text": "How was the Phoenix Criteria Developed\nData from hospital systems in North America, Asia, Africa, and South America, were collected and harmonized into one analysis set of more than 3 million patient encounters. A data-informed Delphi process was used to identify the most useful existing organ dysfunction scoring systems. The term “useful” means that the scoring system preformed well in estimating risk of death in statistical models, but was a reasonable scoring system to use world-wide. For example, a score that considers a specific and expensive laboratory test would not be considered useful since it is unlikely to be available in middle- or low- resourced environments, even if that test was extremely good at predicating mortality in statistical models.\nAfter a set of useful organ dysfunction scores were identified, stacked regression methods were used to test different combinations of the scores for predicting mortality. Finally, a set of organ dysfunction scores were selected by the Delphi-process and the Phoenix Criteria was established."
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html#the-phoenix-criteria",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html#the-phoenix-criteria",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "The Phoenix Criteria",
    "text": "The Phoenix Criteria\nThe reader is encouraged to reference the published papers for all the details.\nThe Phoenix criteria is based on dysfunction in four organ systems\n\nrespiratory,\ncardiovascular,\ncoagulation, and\nneurologic.\n\nPoints are given based on the level of dysfunction in each of the systems. 0-3 points for respiratory, 0-6 points for cardiovascular, 0-2 points for coagulation, and 0-2 points for neurologic. A total score of 2 or more points is the diagnostic criteria for sepsis, and a total score of 2 or more points with at least one point in the cardiovascular system is the diagnostic criteria for septic shock.\nNote: An extended scoring system using eight organ systems, the four listed above along with endocrine, immunologic, renal, and hepatic, has been published too."
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html#links-to-the-manuscripts",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html#links-to-the-manuscripts",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "Links to the Manuscripts:",
    "text": "Links to the Manuscripts:\n\nInternational Consensus Criteria for Pediatric Sepsis and Septic Shock\nDevelopment and Validation of the Phoenix Criteria for Pediatric Sepsis and Septic Shock"
  },
  {
    "objectID": "posts/2024-01-21-phoenix-sepsis/index.html#other-links-and-news-posts-about-this-work",
    "href": "posts/2024-01-21-phoenix-sepsis/index.html#other-links-and-news-posts-about-this-work",
    "title": "A New Era For Diagnosis and Research in Pediatric Sepsis",
    "section": "Other links and news posts about this work:",
    "text": "Other links and news posts about this work:\n\nCU Researchers Unveil Modernized Criteria For Pediatric Sepsis and Septic Shock\nYouTube video of the presentation to SCCM: Announcement of the Novel Phoenix Pediatric Sepsis Criteria\nR package, Python Module, and SQL for applying the Phoenix criteria to data sets:\n\nJAMIA Open\nphoenix R package"
  },
  {
    "objectID": "posts/2017-04-10-phd-defended/index.html",
    "href": "posts/2017-04-10-phd-defended/index.html",
    "title": "Ph.D. Dissertation Defended",
    "section": "",
    "text": "I have successfully defended my Ph.D. dissertation: “Parsimonious B-Spline Regression Models via Control Polygon and Control Net Reduction for Identifying Factors Explaining Variation in Daily Hormone Profiles During The Menopausal Transition.”\nOnly minor edits to make to the dissertation itself. Several publications are forthcoming. You can get more info on the control polygon and control net reduction methods from the R package cpr available on  CRAN ,  github.\nupdate: 2024-10-01 A website for the cpr package can be found here."
  },
  {
    "objectID": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html",
    "href": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html",
    "title": "Secondary Axis in ggplot2 v2.2.0",
    "section": "",
    "text": "The recent release of Hadley Whickham’s ggplot2 version 2.2.0 included several new features. Notably\nWhat jumped out to me was the not in the first four points noted above, but a new option to display a secondary axis via sec.axis. The note and example in the RStudio blog post shows a secondary axis which is a one-to-one transformation of the primary axis.\nThis blog post covers my exploration of this tool as I have an immediate want, the sec.axis option to plot secondary axes. In the figure below I have plotted several B-spline basis functions. On the left side there x-axis has two labels, the knot sequence denoted with subscripted ξs and general sequence notation. Below each knot or subsequence of knots, are the numeric values for the knots. The graphic on the right side shows the same basis but uses the sec.axis option to plot the numeric values of the knots on top of the graphic and the knot sequence as the primary axis on the bottom.\nThe following are more details on the construction of these plots."
  },
  {
    "objectID": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#basic-plot-of-a-b-spline-basis",
    "href": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#basic-plot-of-a-b-spline-basis",
    "title": "Secondary Axis in ggplot2 v2.2.0",
    "section": "Basic plot of a B-spline Basis",
    "text": "Basic plot of a B-spline Basis\nMy Ph.D. work involves the use of B-splines. I’ve had to plot the basis functions and the show the location of the knot sequence many times. Here is a very simple example. We will plot the basis for a standard cubic B-spline with knot sequence ξ = {0, 0, 0, 0, 1.2, 3.1, 4.6, 5.1, 6, 6, 6, 6}.\nlibrary(splines)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nxvec   &lt;- seq(0, 6, length = 100)  # data\niknots &lt;- c(1.2, 3.1, 4.6, 5.1)    # internal knots\nbknots &lt;- range(xvec)              # boundary knots\nk      &lt;- 4                        # spline order (degree + 1)\nxi     &lt;- c(rep(bknots[1], k), iknots, rep(bknots[2], k)) # full knot sequence\nbmat   &lt;- bs(xvec, knots = iknots, intercept = TRUE) # basis matrix\nThe easiest plot of the spline functions is done via matplot.\nmatplot(bmat, type = \"l\")\n\nThis plot is okay. My major concern is that x-axis is an index, not the xvec values. Also, all my other graphics are generated via ggplot2 and therefore the basis plot should be too. A simple ggplot2 version of the basis plot is\n# NOTE: as.data.frame is easier than dplyr::as_data_frame for this task\nbmat_df &lt;-\n  bmat %&gt;%\n  as.data.frame %&gt;%\n  add_column(x = xvec) %&gt;%\n  gather(spline, value, -x)\n\nbase_plot &lt;-\n  ggplot(bmat_df) +\n  aes(x = x, y = value, color = spline) +\n  geom_line()\n\nbase_plot"
  },
  {
    "objectID": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#show-the-location-of-the-knots",
    "href": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#show-the-location-of-the-knots",
    "title": "Secondary Axis in ggplot2 v2.2.0",
    "section": "Show the location of the knots",
    "text": "Show the location of the knots\nTo show the location of the knots on the x-axis is easy enough:\nbase_plot + scale_x_continuous(breaks = unique(xi))\n\nWhat I really needed to do was show the location of the knots numerically and using subscripted ξj for simple knots and general sequence notation for repeated knots.\nThis is done by using the atop and group options within plotmath to generate the needed expression to be used in the x-axis ticks. The construction of the expressions is as follows:\nexpr &lt;- list(bquote(atop(group('{', xi[j], '}')[j == 1]^{.(k)},\n                         .(formatC(bknots[1], 1, format = \"f\")))))\n\nif (length(xi) &gt; 2 * k) {\n  for(i in seq(k + 1, length(xi) - k, by = 1)) {\n    expr &lt;- c(expr, bquote(atop(xi[.(i)], .(formatC(xi[i], 1, format = \"f\")))))\n  }\n}\n\nexpr &lt;- c(expr,\n          bquote(atop(group('{', xi[j], '}')[j == .(length(xi) - k + 1L)]^{.(length(xi))},\n                      .(formatC(bknots[2], 1, format = \"f\")))))\nThe updated plot is:\nbase_plot + scale_x_continuous(breaks = unique(xi), labels = expr)\n\n\n\nplot of chunk old_basis_plot\n\n\nI liked this plot. However, the numeric values are not perfectly aligned. Using phantom within the grouping has proven to be non-trivial. So, I’ve been using this style of plot for awhile now."
  },
  {
    "objectID": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#secondary-axis-in-ggplot2-v2.2.0",
    "href": "posts/2016-11-15-secondary-axis-ggplot2-v2.2.0/index.html#secondary-axis-in-ggplot2-v2.2.0",
    "title": "Secondary Axis in ggplot2 v2.2.0",
    "section": "Secondary Axis in ggplot2 v2.2.0",
    "text": "Secondary Axis in ggplot2 v2.2.0\nFirst, let’s clean up the expr to only have the knot sequence characters. No need for the atop as we only need the ξ characters and sequence notation no numeric values.\nexpr &lt;- list(bquote(group('{', xi[j], '}')[j == 1]^{.(k)}))\n\nif (length(xi) &gt; 2 * k) {\n  for(i in seq(k + 1, length(xi) - k, by = 1)) {\n    expr &lt;- c(expr, bquote(xi[.(i)]))\n  }\n}\n\nexpr &lt;- c(expr,\n          bquote(group('{', xi[j], '}')[j == .(length(xi) - k + 1L)]^{.(length(xi))}))\nAdd the primary and secondary axis to base_plot via the argument sec.axis and the function sec_axis.\nbase_plot +\nscale_x_continuous(breaks = unique(xi), labels = expr,\n                   sec.axis = sec_axis(~ ., breaks = unique(xi)))\n\n\n\nplot of chunk new_basis_plot\n\n\nThe first argument to sec.axis is a transform of the primary access values. ~ . is a identity transform. Other arguments sec.axis can take are name, breaks and labels, all of which behave as expected.\nFrom my point of view, reporting both the knot sequence and the numeric values is too much. One or the other should be sufficient. However, my opinion only counts for so much. In my forthcoming cpr package there is a plotting method for B-spline bases with options to show_x and show_xi to control how knots locations are shown in the plot. If both options are FALSE then the default x-axis is plotted. Prior to ggplot2_2.2.0 being released, the options would control the expr object to plot the x-axis on the bottom. Now, with ggplot2_2.2.0 I plan to move away from atop and use sec.axis instead to give the end user the option to plot just the ξs, just the numeric values, or both.\n\nSession Info\nprint(sessionInfo(), local = FALSE)\n## R version 3.3.2 (2016-10-31)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Debian GNU/Linux 8 (jessie)\n##\n## attached base packages:\n## [1] splines   stats     graphics  grDevices utils     datasets  base\n##\n## other attached packages:\n## [1] ggplot2_2.2.0 tidyr_0.6.0   dplyr_0.5.0   tibble_1.2    knitr_1.15\n##\n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.7      digest_0.6.10    assertthat_0.1   plyr_1.8.4\n##  [5] grid_3.3.2       R6_2.2.0         gtable_0.2.0     DBI_0.5-1\n##  [9] magrittr_1.5     evaluate_0.10    scales_0.4.1     highr_0.6\n## [13] stringi_1.1.2    lazyeval_0.2.0   labeling_0.3     tools_3.3.2\n## [17] stringr_1.1.0    munsell_0.4.3    colorspace_1.3-0 gridExtra_2.2.1\n## [21] methods_3.3.2"
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html",
    "title": "Updating Call Arguments",
    "section": "",
    "text": "The stats::update function is one of my favorite tools in R. Using this function saves a lot of time and effort when needing to modify an object. However, this function has its limits. The following is an example of how to extend the use of stats::update.\nThis post is an extension of the lightening talk I gave to the Denver R Users Group in March of 2016. You can get the slides from that talk from my github page."
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#basics-of-the-statsupdate-function",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#basics-of-the-statsupdate-function",
    "title": "Updating Call Arguments",
    "section": "Basics of the stats::update function",
    "text": "Basics of the stats::update function\nThe documentation for the stats::update function is a good starting place. Of course, examples are even better. We’ll use the diamonds data set from within the ggplot2 package for the examples. By default, the cut, color, and clarity elements of the diamonds data set are ordered factors. I’m going to remove the order and level these variables as just factors.\ndata(\"diamonds\", package = \"ggplot2\")\n\ndiamonds$cut     &lt;- factor(diamonds$cut, ordered = FALSE)\ndiamonds$color   &lt;- factor(diamonds$color, ordered = FALSE)\ndiamonds$clarity &lt;- factor(diamonds$clarity, ordered = FALSE)\n\ndplyr::glimpse(diamonds)\n## Observations: 53,940\n## Variables: 10\n## $ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ...\n## $ cut     &lt;fctr&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very ...\n## $ color   &lt;fctr&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J,...\n## $ clarity &lt;fctr&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, S...\n## $ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ...\n## $ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54...\n## $ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,...\n## $ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ...\n## $ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ...\n## $ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ...\nA simple regression model, a lm object, will be used for our examples. Let’s regress the price of diamonds as a function of carat, cut, color, and clarity.\noriginal_fit &lt;- lm(price ~ carat + cut + color + clarity, data = diamonds)"
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#updating-a-call",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#updating-a-call",
    "title": "Updating Call Arguments",
    "section": "Updating a call",
    "text": "Updating a call\nThe stats::update function works by calling getCall which calls getElement. While that is nice to know, the take away message is that if you have an object with an element called call, then stat::update gives you assess to modify the call.\nLet’s update the regression formula. Say instead of the four predictors we started with we want to regress price on the depth and table of the diamonds\nupdated_fit_1 &lt;- update(original_fit, formula = . ~ depth + table)\nIn the code chunk above, the update call took the original_fit object as its first argument and then we provided the formula = . ~ depth + table argent to tell the interpreter that we want to modify the formula. The . is shorthand for “the current,” i.e., use the current left hand side of the formula, and replace the right hand side with the depth + table.\nUsing getCall we can see that the two objects have different calls, and the calls we would expect them to have.\ngetCall(original_fit)\n## lm(formula = price ~ carat + cut + color + clarity, data = diamonds)\ngetCall(updated_fit_1)\n## lm(formula = price ~ depth + table, data = diamonds)\nOne more sanity check: the regression coefficients for these two models are, at least in name, as expected:\ncoef(original_fit)\n##  (Intercept)        carat      cutGood cutVery Good   cutPremium\n##   -7362.8022    8886.1289     655.7674     848.7169     869.3959\n##     cutIdeal       colorE       colorF       colorG       colorH\n##     998.2544    -211.6825    -303.3100    -506.1995    -978.6977\n##       colorI       colorJ   claritySI2   claritySI1   clarityVS2\n##   -1440.3019   -2325.2224    2625.9500    3573.6880    4217.8291\n##   clarityVS1  clarityVVS2  clarityVVS1    clarityIF\n##    4534.8790    4967.1994    5072.0276    5419.6468\ncoef(updated_fit_1)\n##  (Intercept)        depth        table\n## -15084.96675     82.26159    242.58346\nWe can update more than just the formula in the call. Perhaps you want to fit the same regression model on a subset of the data, for example, use the same regression formula as in original_fit but subset the data to only diamonds with a carat weight under 2.\nupdated_fit_2 &lt;- update(original_fit, data = dplyr::filter(diamonds, carat &lt; 2))\ngetCall(updated_fit_2)\n## lm(formula = price ~ carat + cut + color + clarity, data = dplyr::filter(diamonds,\n##     carat &lt; 2))\n\ncoef(original_fit)\n##  (Intercept)        carat      cutGood cutVery Good   cutPremium\n##   -7362.8022    8886.1289     655.7674     848.7169     869.3959\n##     cutIdeal       colorE       colorF       colorG       colorH\n##     998.2544    -211.6825    -303.3100    -506.1995    -978.6977\n##       colorI       colorJ   claritySI2   claritySI1   clarityVS2\n##   -1440.3019   -2325.2224    2625.9500    3573.6880    4217.8291\n##   clarityVS1  clarityVVS2  clarityVVS1    clarityIF\n##    4534.8790    4967.1994    5072.0276    5419.6468\ncoef(updated_fit_2)\n##  (Intercept)        carat      cutGood cutVery Good   cutPremium\n##   -6162.1964    8724.8278     527.6717     712.9909     744.0746\n##     cutIdeal       colorE       colorF       colorG       colorH\n##     861.5481    -223.8747    -313.4734    -508.2788    -996.4275\n##       colorI       colorJ   claritySI2   claritySI1   clarityVS2\n##   -1492.3535   -2210.0376    1635.1460    2580.1405    3240.4271\n##   clarityVS1  clarityVVS2  clarityVVS1    clarityIF\n##    3568.3900    3999.7614    4094.5453    4437.4743\nA second way to achieve the same results as seen with updated_fit_2 is to use the update function to add to the call:\nupdated_fit_3 &lt;- update(original_fit, subset = carat &lt; 2)\ngetCall(updated_fit_3)\n## lm(formula = price ~ carat + cut + color + clarity, data = diamonds,\n##     subset = carat &lt; 2)\n\ncoef(original_fit)\n##  (Intercept)        carat      cutGood cutVery Good   cutPremium\n##   -7362.8022    8886.1289     655.7674     848.7169     869.3959\n##     cutIdeal       colorE       colorF       colorG       colorH\n##     998.2544    -211.6825    -303.3100    -506.1995    -978.6977\n##       colorI       colorJ   claritySI2   claritySI1   clarityVS2\n##   -1440.3019   -2325.2224    2625.9500    3573.6880    4217.8291\n##   clarityVS1  clarityVVS2  clarityVVS1    clarityIF\n##    4534.8790    4967.1994    5072.0276    5419.6468\nall.equal(coef(updated_fit_3), coef(updated_fit_2))\n## [1] TRUE\nLastly, before we move onto more interesting examples, you can always update more than one part of a call with one update call\nupdated_fit_4 &lt;- update(original_fit,\n                        formula = . ~ depth,\n                        data    = dplyr::filter(diamonds, carat &lt; 2),\n                        subset  = cut %in% c(\"Good\", \"Very Good\"))\ngetCall(updated_fit_4)\n## lm(formula = price ~ depth, data = dplyr::filter(diamonds, carat &lt;\n##     2), subset = cut %in% c(\"Good\", \"Very Good\"))\nsummary(updated_fit_4)\n##\n## Call:\n## lm(formula = price ~ depth, data = dplyr::filter(diamonds, carat &lt;\n##     2), subset = cut %in% c(\"Good\", \"Very Good\"))\n##\n## Residuals:\n##     Min      1Q  Median      3Q     Max\n## -3316.1 -2596.6  -954.8  1364.0 15272.6\n##\n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept)  5027.60     934.21   5.382 7.48e-08 ***\n## depth         -24.33      15.07  -1.615    0.106\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## Residual standard error: 3187 on 16321 degrees of freedom\n## Multiple R-squared:  0.0001598,  Adjusted R-squared:  9.852e-05\n## F-statistic: 2.608 on 1 and 16321 DF,  p-value: 0.1063"
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#modifying-a-variable-on-the-right-hand-side",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#modifying-a-variable-on-the-right-hand-side",
    "title": "Updating Call Arguments",
    "section": "Modifying a Variable on the Right Hand Side",
    "text": "Modifying a Variable on the Right Hand Side\nLet’s work with the original_fit object again and modify the right hand side of the regression formula to use a centered and scaled version of carat. (You can quickly center and scale a variable by calling scale. The default behavior is to center and scale by subtracting off the mean and dividing by the standard deviation.) If you attempt to update the formula as . ~ . + scale(carat) where the . is a reuse operator, the result will be nonsensical.\nscaled_fit_1 &lt;- update(original_fit, formula = . ~ . + scale(carat))\ngetCall(scaled_fit_1)\n## lm(formula = price ~ carat + cut + color + clarity + scale(carat),\n##     data = diamonds)\ncoef(scaled_fit_1)\n##  (Intercept)        carat      cutGood cutVery Good   cutPremium\n##   -7362.8022    8886.1289     655.7674     848.7169     869.3959\n##     cutIdeal       colorE       colorF       colorG       colorH\n##     998.2544    -211.6825    -303.3100    -506.1995    -978.6977\n##       colorI       colorJ   claritySI2   claritySI1   clarityVS2\n##   -1440.3019   -2325.2224    2625.9500    3573.6880    4217.8291\n##   clarityVS1  clarityVVS2  clarityVVS1    clarityIF scale(carat)\n##    4534.8790    4967.1994    5072.0276    5419.6468           NA\nNote that carat appears twice in the right hand side of the formula. Further, the regression coefficient for scale(carat) is NA as this vector in the design matrix is a linear function of carat. We’ve created a regression model with a rank deficient design matrix. Oops.\nObviously, we need to omit carat and replace with scale(carat) in the updated formula. Two ways to do this.\n\nDon’t use the . on the right hand side and write out the full right hand side yourself. This option requires too much effort and would suck to maintain.\nContinue to use the . on the right hand side and omit carat via -\n\nscaled_fit_2 &lt;- update(original_fit, formula = . ~ . - carat + scale(carat))\ngetCall(scaled_fit_2)\n## lm(formula = price ~ cut + color + clarity + scale(carat), data = diamonds)\ncoef(scaled_fit_2)\n##  (Intercept)      cutGood cutVery Good   cutPremium     cutIdeal\n##    -272.2067     655.7674     848.7169     869.3959     998.2544\n##       colorE       colorF       colorG       colorH       colorI\n##    -211.6825    -303.3100    -506.1995    -978.6977   -1440.3019\n##       colorJ   claritySI2   claritySI1   clarityVS2   clarityVS1\n##   -2325.2224    2625.9500    3573.6880    4217.8291    4534.8790\n##  clarityVVS2  clarityVVS1    clarityIF scale(carat)\n##    4967.1994    5072.0276    5419.6468    4212.1250\nCool. That worked well.\nNow, what if we wanted to only center carat instead of centering and scaling? This would require adding scale(carat, scale = FALSE) to the right hand side of the formula. Starting with the scaled_fit_2 object we find that this task can be difficult as the full text of scale(carat) needs to be omitted. In the following chunk you’ll see that scaled_fit_3 does not have the desired formula whereas scaled_fit_4 does.\nscaled_fit_3 &lt;-\n  update(scaled_fit_2, formula = . ~ . + scale(carat, scale = FALSE))\nscaled_fit_4 &lt;-\n  update(scaled_fit_2, formula = . ~ . - scale(carat) + scale(carat, scale = FALSE))\n\ngetCall(scaled_fit_3)\n## lm(formula = price ~ cut + color + clarity + scale(carat) + scale(carat,\n##     scale = FALSE), data = diamonds)\ngetCall(scaled_fit_4)\n## lm(formula = price ~ cut + color + clarity + scale(carat, scale = FALSE),\n##     data = diamonds)\nOkay, one more problem. Let’s start with scaled_fit_4 and scale, but not center carat. In the chunk below, scaled_fit_5 does not have the desired result, but scaled_fit_6 does.\nscaled_fit_5 &lt;-\n  update(scaled_fit_4,\n         formula = . ~ . - scale(carat) + scale(carat, center = TRUE))\nscaled_fit_6 &lt;-p\n## Error in eval(expr, envir, enclos): object 'p' not found\n  update(scaled_fit_4,\n         formula = . ~ . - scale(carat, scale = FALSE) + scale(carat, center = TRUE))\n##\n## Call:\n## lm(formula = price ~ cut + color + clarity + scale(carat, center = TRUE),\n##     data = diamonds)\n##\n## Coefficients:\n##                 (Intercept)                      cutGood\n##                      -272.2                        655.8\n##                cutVery Good                   cutPremium\n##                       848.7                        869.4\n##                    cutIdeal                       colorE\n##                       998.3                       -211.7\n##                      colorF                       colorG\n##                      -303.3                       -506.2\n##                      colorH                       colorI\n##                      -978.7                      -1440.3\n##                      colorJ                   claritySI2\n##                     -2325.2                       2625.9\n##                  claritySI1                   clarityVS2\n##                      3573.7                       4217.8\n##                  clarityVS1                  clarityVVS2\n##                      4534.9                       4967.2\n##                 clarityVVS1                    clarityIF\n##                      5072.0                       5419.6\n## scale(carat, center = TRUE)\n##                      4212.1\n\ngetCall(scaled_fit_5)\n## lm(formula = price ~ cut + color + clarity + scale(carat, scale = FALSE) +\n##     scale(carat, center = TRUE), data = diamonds)\ngetCall(scaled_fit_6)\n## Error in getCall(scaled_fit_6): object 'scaled_fit_6' not found\nSo, what do you think? The update function is great, but is has some limitations. Imaging if you had a function of a variable with several options, or just one option with a very long value. Update might not be that useful. For example, instead of scaling carat, let’s move it into bins using the cut() function. (The fact that there is a variable and a function both called cut on the right hand side could be confusing. I selected this data set for this example specifically because it had a meaningful variable name of cut. We will see why this is important later.)\ncut_fit_1 &lt;-\n  update(original_fit,\n         formula = . ~ . - carat +\n                       cut(carat,\n                           breaks = seq(0, 5.5, by = 0.5),\n                           right = FALSE)\n         )\nnames(coef(cut_fit_1))\n##  [1] \"(Intercept)\"\n##  [2] \"cutGood\"\n##  [3] \"cutVery Good\"\n##  [4] \"cutPremium\"\n##  [5] \"cutIdeal\"\n##  [6] \"colorE\"\n##  [7] \"colorF\"\n##  [8] \"colorG\"\n##  [9] \"colorH\"\n## [10] \"colorI\"\n## [11] \"colorJ\"\n## [12] \"claritySI2\"\n## [13] \"claritySI1\"\n## [14] \"clarityVS2\"\n## [15] \"clarityVS1\"\n## [16] \"clarityVVS2\"\n## [17] \"clarityVVS1\"\n## [18] \"clarityIF\"\n## [19] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[0.5,1)\"\n## [20] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[1,1.5)\"\n## [21] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[1.5,2)\"\n## [22] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[2,2.5)\"\n## [23] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[2.5,3)\"\n## [24] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[3,3.5)\"\n## [25] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[3.5,4)\"\n## [26] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[4,4.5)\"\n## [27] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[4.5,5)\"\n## [28] \"cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)[5,5.5)\"\nNow, we have a regression model for price with the cut, color, and clarity accounted for, and a 11 level factor for carat, the interval [0, 0.5) is the reference level here.\nIf you only had the cut_fit_1 object to start with, using the update function to modify the options passed to cut() would be a pain. Having to type out the old cut() call exactly as provide and then replace with a new cut() call. This is too much work and a pain to maintain. Just start with a fresh call to lm. Or, let’s be clever and build some new tools to do this work."
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#modifying-a-call-within-a-formula",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#modifying-a-call-within-a-formula",
    "title": "Updating Call Arguments",
    "section": "Modifying a call within a formula",
    "text": "Modifying a call within a formula\nFirst, let’s look at the structure of a formula. We’ll use the object f, defined below, as the primary object in this example.\nf &lt;- price ~ color + cut + clarity + cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)\nf\n## price ~ color + cut + clarity + cut(carat, breaks = seq(0, 5.5,\n##     by = 0.5), right = FALSE)\nstr(f)\n## Class 'formula'  language price ~ color + cut + clarity + cut(carat, breaks = seq(0, 5.5, by = 0.5),      right = FALSE)\n##   ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt;\nis.list(f)\n## [1] FALSE\nis.recursive(f)\n## [1] TRUE\nWe have a language object with it an environment attribute. This object is not a list, but it is recursive (“is.recursive(x) returns TRUE if x has a recursive (list-like) structure and FALSE otherwise). So, if we recursively apply as.list to f object we see a controlled deconstruction of the formula object.\nas.list(f)\n## [[1]]\n## `~`\n##\n## [[2]]\n## price\n##\n## [[3]]\n## color + cut + clarity + cut(carat, breaks = seq(0, 5.5, by = 0.5),\n##     right = FALSE)\nlapply(as.list(f), as.list)\n## [[1]]\n## [[1]][[1]]\n## `~`\n##\n##\n## [[2]]\n## [[2]][[1]]\n## price\n##\n##\n## [[3]]\n## [[3]][[1]]\n## `+`\n##\n## [[3]][[2]]\n## color + cut + clarity\n##\n## [[3]][[3]]\n## cut(carat, breaks = seq(0, 5.5, by = 0.5), right = FALSE)\nWhat happens when we apply as.list to the third element of the .Last.value?\nlapply(lapply(as.list(f), as.list)[[3]], as.list)\n## [[1]]\n## [[1]][[1]]\n## `+`\n##\n##\n## [[2]]\n## [[2]][[1]]\n## `+`\n##\n## [[2]][[2]]\n## color + cut\n##\n## [[2]][[3]]\n## clarity\n##\n##\n## [[3]]\n## [[3]][[1]]\n## cut\n##\n## [[3]][[2]]\n## carat\n##\n## [[3]]$breaks\n## seq(0, 5.5, by = 0.5)\n##\n## [[3]]$right\n## [1] FALSE\nLook at the elements of the third element, the cut() call is the first sub-element followed by the arguments x (implicitly) and breaks (explicitly).\nThis is great! If a formula is reconstructed recursively then we can access the arguments of calls within the formula.\nLet’s start building a function to fully reconstruct a formula object into it’s parts.\ndecon &lt;- function(x) {\n  if (is.recursive(x)) {\n    lapply(as.list(x), decon)\n  } else {\n    x\n  }\n}\nThe decon function rips apart any recursive object until only the non-recursive elements remain. Passing f to decon yields:\ndecon(f)\n## [[1]]\n## `~`\n##\n## [[2]]\n## price\n##\n## [[3]]\n## [[3]][[1]]\n## `+`\n##\n## [[3]][[2]]\n## [[3]][[2]][[1]]\n## `+`\n##\n## [[3]][[2]][[2]]\n## [[3]][[2]][[2]][[1]]\n## `+`\n##\n## [[3]][[2]][[2]][[2]]\n## color\n##\n## [[3]][[2]][[2]][[3]]\n## cut\n##\n##\n## [[3]][[2]][[3]]\n## clarity\n##\n##\n## [[3]][[3]]\n## [[3]][[3]][[1]]\n## cut\n##\n## [[3]][[3]][[2]]\n## carat\n##\n## [[3]][[3]]$breaks\n## [[3]][[3]]$breaks[[1]]\n## seq\n##\n## [[3]][[3]]$breaks[[2]]\n## [1] 0\n##\n## [[3]][[3]]$breaks[[3]]\n## [1] 5.5\n##\n## [[3]][[3]]$breaks$by\n## [1] 0.5\n##\n##\n## [[3]][[3]]$right\n## [1] FALSE\nwhere we can see a hierarchy for each element and sub-element. Take careful notice of element [[3]][[3]][[1]]. When the deconstruction of the formula runs into the cut() call, the first element is the name of the call itself, followed by the arguments thereto. This is illustrated again with respect to the seq call.\nNow, before we try to modify any arguments, we need decon to return a formual object. After all, the point of this is to modify a formula. By wrapping the lapply in a as.call if x is recursive we gain the desired behavior.\ndecon &lt;- function(x) {\n  if (is.recursive(x)) {\n    as.call(lapply(as.list(x), decon))\n  } else {\n    x\n  }\n}\ndecon(f)\n## price ~ color + cut + clarity + cut(carat, breaks = seq(0, 5.5,\n##     by = 0.5), right = FALSE)\nall.equal(decon(f), f)\n## [1] TRUE\nWhy? Well, each deconstruction is a list with an operator, a named call, as the first element followed by two arguments. A simple example with addition:\nas.call(list(`+`, 1, 2))\n## .Primitive(\"+\")(1, 2)\n\neval(as.call(list(`+`, 1, 2)))\n## [1] 3\nBy wrapping the lapply in the as.call within the decon function we preserve the unevaluated calls until the end of the recursion when the call is implicitly evaluated.\nThe next step in our journey is to modify decon such that arguments to the cut call can be updated. Specifically, we want to change the value of the breaks argument. Just to be clear, the stats::update function can’t do this.\nupdate(cut(carat, breaks = c(1, 1)), breaks = c(3, 4))\n## Error in cut(carat, breaks = c(1, 1)): object 'carat' not found\nWe are looking for a call named cut, and to modify the breaks argument. Using is.call will differentiate between the cut call and the cut variable.\ndecon &lt;- function(x, nb) {\n  if (is.call(x) && grepl(\"cut\", deparse(x[[1]]))) {\n    x$breaks &lt;- nb\n    x\n  } else if (is.recursive(x)) {\n    as.call(lapply(as.list(x), decon, nb))\n  } else {\n    x\n  }\n}\nf\n## price ~ color + cut + clarity + cut(carat, breaks = seq(0, 5.5,\n##     by = 0.5), right = FALSE)\ndecon(f, c(0, 3))\n## price ~ color + cut + clarity + cut(carat, breaks = c(0, 3),\n##     right = FALSE)\nWe’re almost there. The return from decon is a formula. However, we have not dealt with the environments. Let’s place decon within another function, call it newbreaks and then handle environments and calls. While not necessary, to make it clear which functions are being called we will give use the name local_decon within the newbreaks function.\nnewbreaks &lt;- function(form, nb) {\n  local_decon &lt;- function(x, nb) {\n    if (is.call(x) && grepl(\"cut\", deparse(x[[1]]))) {\n      x$breaks &lt;- nb\n      x\n    } else if (is.recursive(x)) {\n      as.call(lapply(as.list(x), local_decon, nb))\n    } else {\n      x\n    }\n  }\n\n  out &lt;- lapply(as.list(form), local_decon, nb)\n  out &lt;- eval(as.call(out))\n  environment(out) &lt;- environment(form)\n  out\n}\n\nnewbreaks(f, c(0, 3))\n## price ~ color + cut + clarity + cut(carat, breaks = c(0, 3),\n##     right = FALSE)\nThis is great! Now we are able to modify the breaks within a cut call within a formula! In practice we could do the following:\nnew_fit_1 &lt;-\n  update(cut_fit_1, formula = newbreaks(formula(cut_fit_1), c(0, 1, 2)))\nnew_fit_2 &lt;-\n  update(cut_fit_1, formula = newbreaks(formula(cut_fit_1), c(0, 1, 3, 5)))\nnew_fit_3 &lt;-\n  update(cut_fit_1, formula = newbreaks(formula(cut_fit_1), seq(0, 5.5, by = 1.25)))\n\nnames(coef(new_fit_1))\n##  [1] \"(Intercept)\"\n##  [2] \"cutGood\"\n##  [3] \"cutVery Good\"\n##  [4] \"cutPremium\"\n##  [5] \"cutIdeal\"\n##  [6] \"colorE\"\n##  [7] \"colorF\"\n##  [8] \"colorG\"\n##  [9] \"colorH\"\n## [10] \"colorI\"\n## [11] \"colorJ\"\n## [12] \"claritySI2\"\n## [13] \"claritySI1\"\n## [14] \"clarityVS2\"\n## [15] \"clarityVS1\"\n## [16] \"clarityVVS2\"\n## [17] \"clarityVVS1\"\n## [18] \"clarityIF\"\n## [19] \"cut(carat, breaks = c(0, 1, 2), right = FALSE)[1,2)\"\nnames(coef(new_fit_2))\n##  [1] \"(Intercept)\"\n##  [2] \"cutGood\"\n##  [3] \"cutVery Good\"\n##  [4] \"cutPremium\"\n##  [5] \"cutIdeal\"\n##  [6] \"colorE\"\n##  [7] \"colorF\"\n##  [8] \"colorG\"\n##  [9] \"colorH\"\n## [10] \"colorI\"\n## [11] \"colorJ\"\n## [12] \"claritySI2\"\n## [13] \"claritySI1\"\n## [14] \"clarityVS2\"\n## [15] \"clarityVS1\"\n## [16] \"clarityVVS2\"\n## [17] \"clarityVVS1\"\n## [18] \"clarityIF\"\n## [19] \"cut(carat, breaks = c(0, 1, 3, 5), right = FALSE)[1,3)\"\n## [20] \"cut(carat, breaks = c(0, 1, 3, 5), right = FALSE)[3,5)\"\nnames(coef(new_fit_3))\n##  [1] \"(Intercept)\"\n##  [2] \"cutGood\"\n##  [3] \"cutVery Good\"\n##  [4] \"cutPremium\"\n##  [5] \"cutIdeal\"\n##  [6] \"colorE\"\n##  [7] \"colorF\"\n##  [8] \"colorG\"\n##  [9] \"colorH\"\n## [10] \"colorI\"\n## [11] \"colorJ\"\n## [12] \"claritySI2\"\n## [13] \"claritySI1\"\n## [14] \"clarityVS2\"\n## [15] \"clarityVS1\"\n## [16] \"clarityVVS2\"\n## [17] \"clarityVVS1\"\n## [18] \"clarityIF\"\n## [19] \"cut(carat, breaks = c(0, 1.25, 2.5, 3.75, 5), right = FALSE)[1.25,2.5)\"\n## [20] \"cut(carat, breaks = c(0, 1.25, 2.5, 3.75, 5), right = FALSE)[2.5,3.75)\"\n## [21] \"cut(carat, breaks = c(0, 1.25, 2.5, 3.75, 5), right = FALSE)[3.75,5)\""
  },
  {
    "objectID": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#why-why-would-you-ever-need-this",
    "href": "posts/2016-11-10-updating-arguments-within-a-call-within-a-formula/index.html#why-why-would-you-ever-need-this",
    "title": "Updating Call Arguments",
    "section": "Why, Why would you ever need this?",
    "text": "Why, Why would you ever need this?\nI hope the above examples would have answered this question. If not, here was my motivation. I have been working with B-splines, a lot. My Ph.D. dissertation focuses on B-spline regression models. I needed to be able to update a regression object with a new formula differing only by the internal knot locations within a spline. Originally, this meant using the splines::bs call and adjusting the knots argument while preserving the values passed to the degree, intercept and Boundary.knots arguments. If you are familiar with the splines::bs call then you’ll know that the there are default arguments to each of these after mentioned arguments.\nFurther, the objects I really needed to update where more complex than just an lm object and, as new software goes, had an ever changing API. Construction of calls was a lot of overhead. When the only thing that needed to be updated was one argument in one call within a formula it seemed reasonable to find a solution to do exactly what I needed and no more.\nOnce I publicly release my cpr package you’ll find, if you dig into the source code, functions cpr:::newknots and cpr:::newdfs to be critical functions in the implementation.\nAcknowledgements: I didn’t figure out how to do this completely on my own. I had posed a question on stackoverflow which was the basis for this post and extensions."
  },
  {
    "objectID": "posts/2016-07-16-ibc/index.html",
    "href": "posts/2016-07-16-ibc/index.html",
    "title": "The 28th International Biometric Conference",
    "section": "",
    "text": "Shaking hands with IBS President Elizabeth Thompson after being awarded Best Student Oral Presentation\nThe International Biometric Society (IBS) held the 28th International Biometric Conference 10-15 July, 2016 in Victoria, British Columbia, Canada. I had submitted my some of my dissertation work to this conference and gave an oral presentation: Parsimonious B-spline Regression Models via Control Polygon Reduction. The talk was well received. The International Biometric Society presented me with the Best Student Oral Presentation and the Western North American Region (WNAR) of the IBS, which held its annual meeting in conjunction with the conference, awarded me Distinguished Oral Presentation as part of the student paper competition.\nThese award announcements were also printed in the Biometic Bulletin, vol 33. no. 3 July-September 2016."
  },
  {
    "objectID": "posts/2016-07-16-ibc/index.html#abstract",
    "href": "posts/2016-07-16-ibc/index.html#abstract",
    "title": "The 28th International Biometric Conference",
    "section": "Abstract",
    "text": "Abstract\nThe following is the abstract for the work I presented.\nParsimonious B-Spline Regression Models via Control Polygon Reduction\nTuesday, July 12, 2016 | 2:00 PM – 2:15 PM | Location: Saanich 1-2 (Level 1) Session WNAR Young Investigators 1 Showcase\nPeter DeWitt; Nichole Carlson; Samantha MaWhinney\nBiostatistics and Informatics, University of Colorado Denver, Anschutz Medical Campus, Aurora, Colorado, United States\nB-spline transformations of continuous predictors are commonly used in regression models to estimate a smooth non-linear relationship with the response. The quality of the regression fit is subject to a knot sequence ξ. Selection of a knot sequence is traditionally achieved by choosing between regression models with a varying number of knots, which are placed at the predictor quantiles. AIC or BIC are then used for model selection. It is well known that AIC and BIC can result in big models (i.e. models with a large number of internal knots). If parsimony, minimizing the number of interior knots n(ξ), is important this approach is not desirable. Our goal is to develop an efficient knot selection algorithm that selects models with smaller n(ξ) without sacrificing goodness of fit.\nInstead of focusing on likelihood maximization, we present a knot selection method based on the geometry of the b-spline control polygons (CP). CPs have been used extensively in computer aided graphic design and numeric analysis; primarily for deriving and evaluating B-spline approximations to fit complex shapes measured with little to no noise. Changes in CP provide a useful metric for assessing the influence of a particular knot, which we demonstrate can then be used for smart removal of knots.\nOur control polygon reduction (CPR) algorithm starts with a CP based on an initial ξ with large n(ξ) and knot positions on a fine partition of the predictor. Inspired by Lyche and Mørken (1988), we assess the influence of each knot on CP geometry and omit the knot exerting the least-influence on the CP shape. After a knot omission, the model is refit with the coarsened knot vector. The process continues until all internal knots are removed. The final regression model is selected as the model with the smallest n(ξ) such that a single additional knot has negligible effects on CP geometry.\nWe show that for a wide range of functional shapes, including complex longitudinal hormone data, the CPR algorithm results in a final model with fewer internal knots than models selected via traditional approaches and with negligible differences in the sum of squared residuals. CPR is computationally efficient and provides high quality fits built on low-rank design matrices. The CPR algorithm is an attractive solution for knot selection in a wide range of applications.\nA bibtex entry for this abstract:\n@inproceedings{dewitt2016parsimonious,\n  author = {DeWitt, Peter E. and Carlson, Nichole E. and Samantha MaWhinney},\n  title = {Parsimonious B-spline Regression Models via Control Polygon Reduction},\n  booktitle = {Abstracts for the XXVIIIth International Biometric Conference}, \n  month = {July}, \n  year  = {2016}, \n  organization = {International Biometric Society}, \n  address = {Victoria, British Columbia},\n  url = {http://www.biometricsociety.org/conference-abstracts/2016/},\n  isbn = {978-0-9821919-4-1}\n}"
  },
  {
    "objectID": "posts/2017-04-21-strother-walker-award/index.html",
    "href": "posts/2017-04-21-strother-walker-award/index.html",
    "title": "Strother Walker Award",
    "section": "",
    "text": "The faculty of the Department of Biostatistics and Informatics has awarded me the 2017 Strother Walker Award based on outstanding work on your dissertation, and your many contributions to computational work being done by your fellow classmates.\nStrother H. Walker award. This is given to an outstanding PhD Biostatistics student who has completed the first year and passed the first written Qualifying exam at the PhD level. Preference is given to students who have passed the second written Qualifying exam. It is partially a cash award funded from the interest on a trust set up by Dorothy Walker in honor of Professor Strother Walker, the former chair of the Department of Biometrics."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter E. DeWitt, Ph.D.",
    "section": "",
    "text": "phoenix R package and Python Module\n\n\n\n\n\n\nnews\n\n\nR\n\n\nsepsis\n\n\npython\n\n\nsql\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nPeter E. DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\nSite Rebuild\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 1, 2024\n\n\nPeter E. DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\nA New Era For Diagnosis and Research in Pediatric Sepsis\n\n\n\n\n\n\nnews\n\n\npublications\n\n\nsepsis\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nPeter E. DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\nInstalling Package Dependencies without external http(s) requests\n\n\n\n\n\n\nR\n\n\nprogramming\n\n\npackage development\n\n\n\n\n\n\n\n\n\nFeb 25, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nStrother Walker Award\n\n\n\n\n\n\nawards\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 21, 2017\n\n\nPeter DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\nPh.D. Dissertation Defended\n\n\n\n\n\n\nphd\n\n\nnews\n\n\ncpr\n\n\n\n\n\n\n\n\n\nApr 10, 2017\n\n\n\n\n\n\n\n\n\n\n\n\ngit diff pdfs\n\n\n\n\n\n\ngit\n\n\n\n\n\n\n\n\n\nApr 3, 2017\n\n\nPeter DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\n10th International Conference on Health Informatics\n\n\n\n\n\n\nnews\n\n\nabstract\n\n\nconferences\n\n\n\n\n\n\n\n\n\nFeb 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n(lazy)Loading Cached Chunks into an Interactive R Session\n\n\n\n\n\n\nR\n\n\nknitr\n\n\ncache\n\n\nqwraps2\n\n\n\n\n\n\n\n\n\nJan 3, 2017\n\n\nPeter DeWitt\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline Characteristics Tables with qwraps2\n\n\n\n\n\n\nR\n\n\ndata summaries\n\n\nqwraps2\n\n\n\n\n\n\n\n\n\nDec 19, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSecondary Axis in ggplot2 v2.2.0\n\n\n\n\n\n\nR\n\n\nprogramming\n\n\nggplot2\n\n\nphd\n\n\n\n\n\n\n\n\n\nNov 15, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nUpdating Call Arguments\n\n\n\n\n\n\nR\n\n\nprogramming\n\n\n\n\n\n\n\n\n\nNov 10, 2016\n\n\n\n\n\n\n\n\n\n\n\n\ndewittpe.github.io Set up\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 26, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nThe 28th International Biometric Conference\n\n\nBest Student Oral Presentation\n\n\n\nabstract\n\n\nconferences\n\n\nawards\n\n\ncpr\n\n\nphd\n\n\nnews\n\n\n\n\n\n\n\n\n\nJul 16, 2016\n\n\nPeter E. DeWitt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blog Roll",
    "section": "",
    "text": "R-Bloggers\nRStudio Blog\nThe International Biometric Society\nDepartment of Biomedical Informatics Newsroom"
  },
  {
    "objectID": "posts/2024-10-02-phoenix-r-package/index.html",
    "href": "posts/2024-10-02-phoenix-r-package/index.html",
    "title": "phoenix R package and Python Module",
    "section": "",
    "text": "Earlier this year I published an R package, Python module, and example SQL code for implementing the Phoenix sepsis criteria.\nThe Phoenix criteria is the new standard diagnostic criteria for pediatric sepsis. Papers on the international consensus and development of the Phoenix criteria were published in JAMA in January of 2024.\nThe R package, Python module, and example SQL code, were written for two reasons:\nThere was one task as part of the development of Phoenix that I was frustrated by–implementing existing organ dysfunction and sepsis scores. The implementation wasn’t difficult from a coding point-of-view, it was difficult in understanding the edge cases which tended to not be well defined in the publications. In my discussions with clinical experts, I found that what was completely clear to the clinician was marginally ambiguous to the person writing the code.\nAn example of this ambiguity between clinician and coder was in the implementation of the Lactate scoring from the PELOD-2 which was ultimately adapted as part of the Phoenix criteria.\nBelow is PELOD-2 reported criteria (see table 1 in the published manuscript) along with a couple alternative notations.\nA couple things I want to point out. The different between 0 and 1 point implicitly defines 5 mmol/L as 1 point due to the strict inequality for 0 points. That’s fine, and quite common in the literature for other organ dysfunction scores. The difference between 1 and 2 points is mildly ambiguous. Given that 2 points is defined as greater than or equal to 11 mmol/L then the implication that strictly less than 11 mmol/L (and greater or equal to 5 mmol/L) is 1 point. So yes, the PELOD-2 definition is complete, but it requires a little bit of active and critical thinking to correctly implement.\nIn my discussions with clinicians, Lactate is only really known, at least in their experience, to one decimal place and a negative value would just be ignored, as would an implausibly large value. So, at the bedside, the definition is clear and complete.\nHowever, in the data set we had form 10 hospital system from North America, South America, Asia, and Africa, the majority only reported Lactate to one decimal point. However, there were some hospital systems, and many reported lab results were Lactate \\(\\in (10.9, 11.0)\\) mmol/L. What are the rounding rules to use?\nTo alleviate the ambiguity and provide the needed detail to the analysis with zero medical knowledge, I prefer the alternative 2 notation. This is complete and covers all possible values, cut points, and even impossible values.\nThat said, the need to have the explicit lower bound is not necessary. The development of Phoenix mapped missing values, a negative Lactate is not possible and could be considered a missing value, map to scores of zero. So alternative 1 might be the best notation, for the scoring. Alternative 1 has the advantage of being complete in that someone could implement the scoring for 1 point with out having to know anything about the scoring rules for 0 or 2 points.\nThat all said, the actual implementation for the lactate scores is extremely simple:\nOkay, stepping off my ridicules soap box. The table published in the PELOD-2 manuscript and the JAMA publications for Phoenix are sufficient and consistent with other published organ dysfunction scoring tables. I just didn’t like having to think so hard and try to find additional detail to know if I was correctly rounding, or working with the cut points as intended.\nAs one of the primary data analysts on the team that developed the Phoenix score, I know exactly how the scoring criteria were implemented. By providing a code base that implements the scoring consistent with development scoring, any ambiguity in the published tables can be ignored as the code will just take care of it."
  },
  {
    "objectID": "posts/2024-10-02-phoenix-r-package/index.html#the-phoenix-r-package",
    "href": "posts/2024-10-02-phoenix-r-package/index.html#the-phoenix-r-package",
    "title": "phoenix R package and Python Module",
    "section": "The phoenix R package",
    "text": "The phoenix R package\nAt the time of writing this post, version 1.1.0 of the phoenix package is available on CRAN.\n\nlibrary(phoenix)\n\nWithin the package there is an example data set called “sepsis” used for examples. It is a data.frame with 20 rows of 27 variables. Review the documentation for details on each of the variables.\n\n?sepsis\n\nHere is an example of applying the cardiovascular scoring. The score ranges from 0 to 6 points and considers three areas.\nFirst, is the patient on any vasocative medications? 0 points for no medications, 1 point for one medication, and 2 points for two or more medications. The set of possible medications is dobutamine, dopamine, epinephrine, milrinone, norepinephrine, and vasopressin. A notable difference from Phoenix and other cardiovascular dysfunction scores based on medications, is that the dosage and the specific medication is not relevant to Phoenix. The reason for this is that the Phoenix criteria needed to be applicable to high- and middle- or low-resourced environments. Some places may not have access to all six medications, in our development set there was a site with only three of the six, a couple sites with only four of the six. If the specific medications were considered then the sites that did not have the medication would artificially have less ill patients. Phoenix avoids this problem by just counting the medications. Additionally, other cardiovascular dysfunction scores considered the dosage of the medications. The units for the dosage, generally μg/kg/min, was not consistently report either.\nNext, lactate values contribute 0, 1, or 2 points, as noted above.\nLastly, age adjusted mean arterial pressures (MAP) contributes another 0, 1, or 2 points.\nFull details on the scoring can be found in the documentation for the phoenix_cardiovascular scoring function and the published manuscripts.\n?phoenix_cardiovascular\n\ncard_example &lt;-\n  sepsis[c(\"pid\", \"dobutamine\", \"dopamine\", \"epinephrine\", \"milrinone\",\n           \"norepinephrine\", \"vasopressin\", \"lactate\", \"dbp\", \"sbp\", \"age\")]\n\ncard_example$score &lt;-\n  phoenix_cardiovascular(\n    vasoactives = dobutamine + dopamine + epinephrine +\n                  milrinone + norepinephrine + vasopressin,\n    lactate     = lactate,\n    age         = age,\n    map         = map(sbp = sbp, dbp = dbp),\n    data        = sepsis)\n\ncard_example\n\n   pid dobutamine dopamine epinephrine milrinone norepinephrine vasopressin\n1    1          1        1           1         1              0           0\n2    2          0        1           0         0              1           0\n3    3          0        1           0         0              0           0\n4    4          0        0           0         0              0           0\n5    5          0        0           0         0              0           0\n6    6          0        1           0         0              0           0\n7    7          0        0           1         1              0           1\n8    8          0        0           0         0              0           0\n9    9          0        0           1         1              1           1\n10  10          0        0           0         0              0           0\n11  11          0        1           1         0              0           0\n12  12          0        0           0         0              0           0\n13  13          0        0           0         0              0           0\n14  14          0        0           1         1              0           0\n15  15          0        1           1         1              0           1\n16  16          1        1           1         1              1           0\n17  17          0        1           1         1              0           1\n18  18          0        1           1         1              0           1\n19  19          0        0           1         1              0           0\n20  20          0        0           1         0              0           0\n   lactate dbp sbp    age score\n1       NA  40  53   0.06     2\n2     3.32  60  90 201.70     2\n3     1.00  87 233  20.80     1\n4       NA  57 104 192.50     0\n5       NA  57 101 214.40     0\n6     1.15  79 119 101.20     1\n7       NA  11  14 150.70     4\n8       NA  66 112 159.70     0\n9     8.10  51 117 176.10     3\n10      NA  58  84   6.60     0\n11      NA  39  51  36.70     3\n12      NA  63 132  37.40     0\n13      NA  55  93   0.12     0\n14      NA  54 106  62.30     2\n15      NA  25  37  10.60     3\n16    0.90  55  82   0.89     2\n17    0.60  43  79  10.70     2\n18      NA  53  75  10.60     2\n19      NA  44  70   0.17     2\n20    2.20  77  99  71.90     1\n\n\nTo apply the full Phoenix criteria to a data set, scoring based on respiratory, cardiovascular, coagulation, and neurologic dysfunction, can be done with a call to phoenix.\n\nphoenix_scores &lt;-\n  phoenix(\n    # respiratory\n      pf_ratio = pao2 / fio2,\n      sf_ratio = ifelse(spo2 &lt;= 97, spo2 / fio2, NA_real_),\n      imv = vent,\n      other_respiratory_support = as.integer(fio2 &gt; 0.21),\n    # cardiovascular\n      vasoactives = dobutamine + dopamine + epinephrine + milrinone + norepinephrine + vasopressin,\n      lactate = lactate,\n      age = age,\n      map = map(sbp, dbp),\n    # coagulation\n      platelets = platelets,\n      inr = inr,\n      d_dimer = d_dimer,\n      fibrinogen = fibrinogen,\n    # neurologic\n      gcs = gcs_total,\n      fixed_pupils = as.integer(pupil == \"both-fixed\"),\n    data = sepsis\n  )\nstr(phoenix_scores)\n\n'data.frame':   20 obs. of  7 variables:\n $ phoenix_respiratory_score   : int  0 3 3 0 0 3 3 0 3 3 ...\n $ phoenix_cardiovascular_score: int  2 2 1 0 0 1 4 0 3 0 ...\n $ phoenix_coagulation_score   : int  1 1 2 1 0 2 2 1 1 0 ...\n $ phoenix_neurologic_score    : int  0 1 0 0 0 1 0 0 1 1 ...\n $ phoenix_sepsis_score        : int  3 7 6 1 0 7 9 1 8 4 ...\n $ phoenix_sepsis              : int  1 1 1 0 0 1 1 0 1 1 ...\n $ phoenix_septic_shock        : int  1 1 1 0 0 1 1 0 1 0 ...\n\n\nThe return from phoenix is a data.frame with the individual organ dysfunction scores, the total Phoenix score, an indicator for Phoenix Sepsis (a score of 2 or more points), and an indicator for Phoenix septic shock (a score of 2 or more points with at least one cardiovascular dysfunction point).\nAdditionally there is a Phoenix-8 scoring which extends the Phoenix scoring to include endocrine, immunologic, renal, and hepatic organ dysfunction scores and is implemented in the function phoenix8.\n?phoenix8"
  },
  {
    "objectID": "posts/2024-10-02-phoenix-r-package/index.html#python-module",
    "href": "posts/2024-10-02-phoenix-r-package/index.html#python-module",
    "title": "phoenix R package and Python Module",
    "section": "Python module",
    "text": "Python module\nA python module is available on PyPi. It mirrors the function of the R package closely and contains the same example data set.\n\nimport numpy as np\nimport pandas as pd\nimport importlib.resources\nimport phoenix as phx\n\npath = importlib.resources.files('phoenix')\nsepsis = pd.read_csv(path.joinpath('data').joinpath('sepsis.csv'))\n\nScoring the cardiovascular dysfunction:\n\npy_card = phx.phoenix_cardiovascular(\n    vasoactives = sepsis[\"dobutamine\"] + sepsis[\"dopamine\"] +\n                  sepsis[\"epinephrine\"] + sepsis[\"milrinone\"] +\n                  sepsis[\"norepinephrine\"] + sepsis[\"vasopressin\"],\n    lactate = sepsis[\"lactate\"],\n    age = sepsis[\"age\"],\n    map = phx.map(sepsis[\"sbp\"], sepsis[\"dbp\"])\n)\nprint(type(py_card))\n\n&lt;class 'numpy.ndarray'&gt;\n\nprint(py_card)\n\n[2 2 1 0 0 1 4 0 3 0 3 0 0 2 3 2 2 2 2 1]\n\n\nScoring the Phoenix criteria:\n\npy_phoenix_scores = phx.phoenix(\n    # resp\n    pf_ratio = sepsis[\"pao2\"] / sepsis[\"fio2\"],\n    sf_ratio = np.where(sepsis[\"spo2\"] &lt;= 97, sepsis[\"spo2\"] / sepsis[\"fio2\"], np.nan),\n    imv      = sepsis[\"vent\"],\n    other_respiratory_support = (sepsis[\"fio2\"] &gt; 0.21).astype(int).to_numpy(),\n    # cardio\n    vasoactives = sepsis[\"dobutamine\"] + sepsis[\"dopamine\"] +\n                  sepsis[\"epinephrine\"] + sepsis[\"milrinone\"] +\n                  sepsis[\"norepinephrine\"] + sepsis[\"vasopressin\"],\n    lactate = sepsis[\"lactate\"],\n    age = sepsis[\"age\"],\n    map = phx.map(sepsis[\"sbp\"], sepsis[\"dbp\"]),\n    # coag\n    platelets = sepsis['platelets'],\n    inr = sepsis['inr'],\n    d_dimer = sepsis['d_dimer'],\n    fibrinogen = sepsis['fibrinogen'],\n    # neuro\n    gcs = sepsis[\"gcs_total\"],\n    fixed_pupils = (sepsis[\"pupil\"] == \"both-fixed\").astype(int),\n    )\nprint(py_phoenix_scores.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20 entries, 0 to 19\nData columns (total 7 columns):\n #   Column                        Non-Null Count  Dtype\n---  ------                        --------------  -----\n 0   phoenix_respiratory_score     20 non-null     int64\n 1   phoenix_cardiovascular_score  20 non-null     int64\n 2   phoenix_coagulation_score     20 non-null     int64\n 3   phoenix_neurologic_score      20 non-null     int64\n 4   phoenix_sepsis_score          20 non-null     int64\n 5   phoenix_sepsis                20 non-null     int64\n 6   phoenix_septic_shock          20 non-null     int64\ndtypes: int64(7)\nmemory usage: 1.2 KB\nNone\n\nprint(py_phoenix_scores.head())\n\n   phoenix_respiratory_score  ...  phoenix_septic_shock\n0                          0  ...                     1\n1                          3  ...                     1\n2                          3  ...                     1\n3                          0  ...                     0\n4                          0  ...                     0\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "posts/2024-10-02-phoenix-r-package/index.html#links",
    "href": "posts/2024-10-02-phoenix-r-package/index.html#links",
    "title": "phoenix R package and Python Module",
    "section": "Links:",
    "text": "Links:\n\nPackage website\ngithub\nCRAN\nPyPi\nPeer Reviewed Manuscripts:\n\nphoenix: an R package and Python module for calculating the Phoenix pediatric sepsis score and criteria\nInternational Consensus Criteria for Pediatric Sepsis and Septic Shock\nDevelopment and Validation of the Phoenix Criteria for Pediatric Sepsis and Septic Shock\n\nOther blog posts:\n\nCU Anschutz: Researchers Take Diagnosis of Pediatric Sepsis to the Next Level"
  },
  {
    "objectID": "posts/2024-10-02-phoenix-r-package/index.html#closing",
    "href": "posts/2024-10-02-phoenix-r-package/index.html#closing",
    "title": "phoenix R package and Python Module",
    "section": "Closing",
    "text": "Closing\nIf you are interested in researching pediatric sepsis then the Phoenix criteria will be an important part of your research for the forseeable future. The R pacakge, Python module, and example SQL code have been provided to simplify the implementation of the Phoenix criteria."
  }
]